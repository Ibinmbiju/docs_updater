{
  "markdown": "[コンテンツにスキップ](https://openai.github.io/openai-agents-python/ja/models/#_1)\n\n# モデル\n\nAgents SDK には、標準で 2 種類の OpenAI モデルサポートが含まれています。\n\n- **推奨**: [`OpenAIResponsesModel`](https://openai.github.io/openai-agents-python/ref/models/openai_responses/#agents.models.openai_responses.OpenAIResponsesModel \"OpenAIResponsesModel\") — 新しい [Responses API](https://platform.openai.com/docs/api-reference/responses) を利用して OpenAI API を呼び出します。\n- [`OpenAIChatCompletionsModel`](https://openai.github.io/openai-agents-python/ref/models/openai_chatcompletions/#agents.models.openai_chatcompletions.OpenAIChatCompletionsModel \"OpenAIChatCompletionsModel\") — [Chat Completions API](https://platform.openai.com/docs/api-reference/chat) を利用して OpenAI API を呼び出します。\n\n## モデルの組み合わせ\n\n1 つのワークフロー内で、エージェントごとに異なるモデルを使用したい場合があります。たとえば、振り分けには小さく高速なモデルを、複雑なタスクには大きく高性能なモデルを使う、といった使い分けです。 [`Agent`](https://openai.github.io/openai-agents-python/ja/ref/agent/#agents.agent.Agent \"Agent            dataclass   \") を設定する際は、以下のいずれかで特定のモデルを指定できます。\n\n1. OpenAI モデル名を直接渡す\n2. 任意のモデル名と、それを `Model` インスタンスへマッピングできる [`ModelProvider`](https://openai.github.io/openai-agents-python/ref/models/interface/#agents.models.interface.ModelProvider \"ModelProvider\") を渡す\n3. [`Model`](https://openai.github.io/openai-agents-python/ref/models/interface/#agents.models.interface.Model \"Model\") 実装を直接渡す\n\nNote\n\nSDK は [`OpenAIResponsesModel`](https://openai.github.io/openai-agents-python/ref/models/openai_responses/#agents.models.openai_responses.OpenAIResponsesModel \"OpenAIResponsesModel\") と [`OpenAIChatCompletionsModel`](https://openai.github.io/openai-agents-python/ref/models/openai_chatcompletions/#agents.models.openai_chatcompletions.OpenAIChatCompletionsModel \"OpenAIChatCompletionsModel\") の両方の形に対応していますが、ワークフローごとに 1 つのモデル形を使用することを推奨します。2 つの形ではサポートする機能・ツールが異なるためです。どうしても混在させる場合は、利用するすべての機能が両方で利用可能であることを確認してください。\n\n```md-code__content\nfrom agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\nimport asyncio\n\nspanish_agent = Agent(\n    name=\"Spanish agent\",\n    instructions=\"You only speak Spanish.\",\n    model=\"o3-mini\",\n)\n\nenglish_agent = Agent(\n    name=\"English agent\",\n    instructions=\"You only speak English\",\n    model=OpenAIChatCompletionsModel(\n        model=\"gpt-4o\",\n        openai_client=AsyncOpenAI()\n    ),\n)\n\ntriage_agent = Agent(\n    name=\"Triage agent\",\n    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n    handoffs=[spanish_agent, english_agent],\n    model=\"gpt-3.5-turbo\",\n)\n\nasync def main():\n    result = await Runner.run(triage_agent, input=\"Hola, ¿cómo estás?\")\n    print(result.final_output)\n\n```\n\nエージェントで使用するモデルをさらに細かく設定したい場合は、 `temperature` などのオプションを指定できる [`ModelSettings`](https://openai.github.io/openai-agents-python/ref/model_settings/#agents.model_settings.ModelSettings \"ModelSettings            dataclass   \") を渡します。\n\n```md-code__content\nfrom agents import Agent, ModelSettings\n\nenglish_agent = Agent(\n    name=\"English agent\",\n    instructions=\"You only speak English\",\n    model=\"gpt-4o\",\n    model_settings=ModelSettings(temperature=0.1),\n)\n\n```\n\n## 他の LLM プロバイダーの利用\n\n他の LLM プロバイダーは 3 通りの方法で利用できます（コード例は [こちら](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)）。\n\n1. [`set_default_openai_client`](https://openai.github.io/openai-agents-python/ref/#agents.set_default_openai_client \"set_default_openai_client\")\n\n\n    OpenAI 互換の API エンドポイントを持つ場合に、 `AsyncOpenAI` インスタンスをグローバルに LLM クライアントとして設定できます。 `base_url` と `api_key` を設定するケースです。設定例は [examples/model\\_providers/custom\\_example\\_global.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_global.py)。\n\n2. [`ModelProvider`](https://openai.github.io/openai-agents-python/ref/models/interface/#agents.models.interface.ModelProvider \"ModelProvider\")\n\n`Runner.run` レベルで「この実行中のすべてのエージェントにカスタムモデルプロバイダーを使う」と宣言できます。設定例は [examples/model\\_providers/custom\\_example\\_provider.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_provider.py)。\n\n3. [`Agent.model`](https://openai.github.io/openai-agents-python/ref/agent/#agents.agent.Agent.model \"model            class-attribute       instance-attribute   \")\n\n\n    特定の Agent インスタンスにモデルを指定できます。エージェントごとに異なるプロバイダーを組み合わせられます。設定例は [examples/model\\_providers/custom\\_example\\_agent.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_agent.py)。多くのモデルを簡単に使う方法として [LiteLLM 連携](https://openai.github.io/openai-agents-python/ja/models/litellm/) があります。\n\n\n`platform.openai.com` の API キーを持たない場合は、 `set_tracing_disabled()` でトレーシングを無効化するか、 [別のトレーシングプロセッサー](https://openai.github.io/openai-agents-python/ja/tracing/) を設定することを推奨します。\n\nNote\n\nこれらの例では Chat Completions API/モデルを使用しています。多くの LLM プロバイダーがまだ Responses API をサポートしていないためです。もしプロバイダーが Responses API をサポートしている場合は、Responses の使用を推奨します。\n\n## 他の LLM プロバイダーでよくある問題\n\n### Tracing クライアントの 401 エラー\n\nトレースは OpenAI サーバーへアップロードされるため、OpenAI API キーがない場合にエラーになります。解決策は次の 3 つです。\n\n1. トレーシングを完全に無効化する: [`set_tracing_disabled(True)`](https://openai.github.io/openai-agents-python/ref/#agents.set_tracing_disabled \"set_tracing_disabled\")\n2. トレーシング用の OpenAI キーを設定する: [`set_tracing_export_api_key(...)`](https://openai.github.io/openai-agents-python/ref/#agents.set_tracing_export_api_key \"set_tracing_export_api_key\")\n\n\n    このキーはトレースのアップロードにのみ使用され、 [platform.openai.com](https://platform.openai.com/) のものが必要です。\n3. OpenAI 以外のトレースプロセッサーを使う。詳しくは [tracing ドキュメント](https://openai.github.io/openai-agents-python/ja/tracing/#custom-tracing-processors) を参照してください。\n\n### Responses API サポート\n\nSDK は既定で Responses API を使用しますが、多くの LLM プロバイダーはまだ対応していません。そのため 404 などのエラーが発生する場合があります。対処方法は 2 つです。\n\n1. [`set_default_openai_api(\"chat_completions\")`](https://openai.github.io/openai-agents-python/ref/#agents.set_default_openai_api \"set_default_openai_api\") を呼び出す\n\n\n    環境変数 `OPENAI_API_KEY` と `OPENAI_BASE_URL` を設定している場合に機能します。\n2. [`OpenAIChatCompletionsModel`](https://openai.github.io/openai-agents-python/ref/models/openai_chatcompletions/#agents.models.openai_chatcompletions.OpenAIChatCompletionsModel \"OpenAIChatCompletionsModel\") を使用する\n\n\n    コード例は [こちら](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/) にあります。\n\n### structured outputs のサポート\n\n一部のモデルプロバイダーは [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) をサポートしていません。その場合、次のようなエラーが発生することがあります。\n\n```md-code__content\nBadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n\n```\n\nこれは一部プロバイダーの制限で、JSON 出力はサポートしていても `json_schema` を指定できません。現在修正に取り組んでいますが、JSON スキーマ出力をサポートしているプロバイダーを利用することを推奨します。そうでない場合、不正な JSON によりアプリが頻繁に壊れる可能性があります。",
  "metadata": {
    "viewport": "width=device-width,initial-scale=1",
    "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
    "language": "ja",
    "title": "モデル - OpenAI Agents SDK",
    "favicon": "https://openai.github.io/openai-agents-python/images/favicon-platform.svg",
    "scrapeId": "02b48698-3081-4ba5-b31a-0b8cb32aea4d",
    "sourceURL": "https://openai.github.io/openai-agents-python/ja/models/",
    "url": "https://openai.github.io/openai-agents-python/ja/models/",
    "statusCode": 200,
    "contentType": "text/html; charset=utf-8",
    "proxyUsed": "basic"
  }
}