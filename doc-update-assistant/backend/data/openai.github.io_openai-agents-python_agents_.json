{
  "markdown": "[Skip to content](https://openai.github.io/openai-agents-python/agents/#agents)\n\n# Agents\n\nAgents are the core building block in your apps. An agent is a large language model (LLM), configured with instructions and tools.\n\n## Basic configuration\n\nThe most common properties of an agent you'll configure are:\n\n- `instructions`: also known as a developer message or system prompt.\n- `model`: which LLM to use, and optional `model_settings` to configure model tuning parameters like temperature, top\\_p, etc.\n- `tools`: Tools that the agent can use to achieve its tasks.\n\n```md-code__content\nfrom agents import Agent, ModelSettings, function_tool\n\n@function_tool\ndef get_weather(city: str) -> str:\n    return f\"The weather in {city} is sunny\"\n\nagent = Agent(\n    name=\"Haiku agent\",\n    instructions=\"Always respond in haiku form\",\n    model=\"o3-mini\",\n    tools=[get_weather],\n)\n\n```\n\n## Context\n\nAgents are generic on their `context` type. Context is a dependency-injection tool: it's an object you create and pass to `Runner.run()`, that is passed to every agent, tool, handoff etc, and it serves as a grab bag of dependencies and state for the agent run. You can provide any Python object as the context.\n\n```md-code__content\n@dataclass\nclass UserContext:\n    uid: str\n    is_pro_user: bool\n\n    async def fetch_purchases() -> list[Purchase]:\n        return ...\n\nagent = Agent[UserContext](\n    ...,\n)\n\n```\n\n## Output types\n\nBy default, agents produce plain text (i.e. `str`) outputs. If you want the agent to produce a particular type of output, you can use the `output_type` parameter. A common choice is to use [Pydantic](https://docs.pydantic.dev/) objects, but we support any type that can be wrapped in a Pydantic [TypeAdapter](https://docs.pydantic.dev/latest/api/type_adapter/) \\- dataclasses, lists, TypedDict, etc.\n\n```md-code__content\nfrom pydantic import BaseModel\nfrom agents import Agent\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\nagent = Agent(\n    name=\"Calendar extractor\",\n    instructions=\"Extract calendar events from text\",\n    output_type=CalendarEvent,\n)\n\n```\n\nNote\n\nWhen you pass an `output_type`, that tells the model to use [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) instead of regular plain text responses.\n\n## Handoffs\n\nHandoffs are sub-agents that the agent can delegate to. You provide a list of handoffs, and the agent can choose to delegate to them if relevant. This is a powerful pattern that allows orchestrating modular, specialized agents that excel at a single task. Read more in the [handoffs](https://openai.github.io/openai-agents-python/handoffs/) documentation.\n\n```md-code__content\nfrom agents import Agent\n\nbooking_agent = Agent(...)\nrefund_agent = Agent(...)\n\ntriage_agent = Agent(\n    name=\"Triage agent\",\n    instructions=(\n        \"Help the user with their questions.\"\n        \"If they ask about booking, handoff to the booking agent.\"\n        \"If they ask about refunds, handoff to the refund agent.\"\n    ),\n    handoffs=[booking_agent, refund_agent],\n)\n\n```\n\n## Dynamic instructions\n\nIn most cases, you can provide instructions when you create the agent. However, you can also provide dynamic instructions via a function. The function will receive the agent and context, and must return the prompt. Both regular and `async` functions are accepted.\n\n```md-code__content\ndef dynamic_instructions(\n    context: RunContextWrapper[UserContext], agent: Agent[UserContext]\n) -> str:\n    return f\"The user's name is {context.context.name}. Help them with their questions.\"\n\nagent = Agent[UserContext](\n    name=\"Triage agent\",\n    instructions=dynamic_instructions,\n)\n\n```\n\n## Lifecycle events (hooks)\n\nSometimes, you want to observe the lifecycle of an agent. For example, you may want to log events, or pre-fetch data when certain events occur. You can hook into the agent lifecycle with the `hooks` property. Subclass the [`AgentHooks`](https://openai.github.io/openai-agents-python/ref/lifecycle/#agents.lifecycle.AgentHooks \"AgentHooks\") class, and override the methods you're interested in.\n\n## Guardrails\n\nGuardrails allow you to run checks/validations on user input, in parallel to the agent running. For example, you could screen the user's input for relevance. Read more in the [guardrails](https://openai.github.io/openai-agents-python/guardrails/) documentation.\n\n## Cloning/copying agents\n\nBy using the `clone()` method on an agent, you can duplicate an Agent, and optionally change any properties you like.\n\n```md-code__content\npirate_agent = Agent(\n    name=\"Pirate\",\n    instructions=\"Write like a pirate\",\n    model=\"o3-mini\",\n)\n\nrobot_agent = pirate_agent.clone(\n    name=\"Robot\",\n    instructions=\"Write like a robot\",\n)\n\n```\n\n## Forcing tool use\n\nSupplying a list of tools doesn't always mean the LLM will use a tool. You can force tool use by setting [`ModelSettings.tool_choice`](https://openai.github.io/openai-agents-python/ref/model_settings/#agents.model_settings.ModelSettings.tool_choice \"tool_choice            class-attribute       instance-attribute   \"). Valid values are:\n\n1. `auto`, which allows the LLM to decide whether or not to use a tool.\n2. `required`, which requires the LLM to use a tool (but it can intelligently decide which tool).\n3. `none`, which requires the LLM to _not_ use a tool.\n4. Setting a specific string e.g. `my_tool`, which requires the LLM to use that specific tool.\n\nNote\n\nTo prevent infinite loops, the framework automatically resets `tool_choice` to \"auto\" after a tool call. This behavior is configurable via [`agent.reset_tool_choice`](https://openai.github.io/openai-agents-python/ref/agent/#agents.agent.Agent.reset_tool_choice \"reset_tool_choice            class-attribute       instance-attribute   \"). The infinite loop is because tool results are sent to the LLM, which then generates another tool call because of `tool_choice`, ad infinitum.\n\nIf you want the Agent to completely stop after a tool call (rather than continuing with auto mode), you can set \\[ `Agent.tool_use_behavior=\"stop_on_first_tool\"`\\] which will directly use the tool output as the final response without further LLM processing.",
  "metadata": {
    "title": "Agents - OpenAI Agents SDK",
    "favicon": "https://openai.github.io/openai-agents-python/images/favicon-platform.svg",
    "language": "en",
    "viewport": "width=device-width,initial-scale=1",
    "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
    "scrapeId": "df1ab8d1-8236-49e8-87f6-8822a29dba73",
    "sourceURL": "https://openai.github.io/openai-agents-python/agents/",
    "url": "https://openai.github.io/openai-agents-python/agents/",
    "statusCode": 200,
    "contentType": "text/html; charset=utf-8",
    "proxyUsed": "basic"
  }
}