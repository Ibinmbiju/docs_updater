{
  "markdown": "[コンテンツにスキップ](https://openai.github.io/openai-agents-python/ja/ref/models/interface/#model-interface)\n\n# `Model interface`\n\n### ModelTracing\n\nBases: `Enum`\n\nSource code in `src/agents/models/interface.py`\n\n|     |     |\n| --- | --- |\n| ```<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>``` | ```md-code__content<br>class ModelTracing(enum.Enum):<br>    DISABLED = 0<br>    \"\"\"Tracing is disabled entirely.\"\"\"<br>    ENABLED = 1<br>    \"\"\"Tracing is enabled, and all data is included.\"\"\"<br>    ENABLED_WITHOUT_DATA = 2<br>    \"\"\"Tracing is enabled, but inputs/outputs are not included.\"\"\"<br>    def is_disabled(self) -> bool:<br>        return self == ModelTracing.DISABLED<br>    def include_data(self) -> bool:<br>        return self == ModelTracing.ENABLED<br>``` |\n\n#### DISABLED`class-attribute``instance-attribute`\n\n```md-code__content\nDISABLED = 0\n\n```\n\nTracing is disabled entirely.\n\n#### ENABLED`class-attribute``instance-attribute`\n\n```md-code__content\nENABLED = 1\n\n```\n\nTracing is enabled, and all data is included.\n\n#### ENABLED\\_WITHOUT\\_DATA`class-attribute``instance-attribute`\n\n```md-code__content\nENABLED_WITHOUT_DATA = 2\n\n```\n\nTracing is enabled, but inputs/outputs are not included.\n\n### Model\n\nBases: `ABC`\n\nThe base interface for calling an LLM.\n\nSource code in `src/agents/models/interface.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>``` | ```md-code__content<br>class Model(abc.ABC):<br>    \"\"\"The base interface for calling an LLM.\"\"\"<br>    @abc.abstractmethod<br>    async def get_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchemaBase | None,<br>        handoffs: list[Handoff],<br>        tracing: ModelTracing,<br>        *,<br>        previous_response_id: str | None,<br>        prompt: ResponsePromptParam | None,<br>    ) -> ModelResponse:<br>        \"\"\"Get a response from the model.<br>        Args:<br>            system_instructions: The system instructions to use.<br>            input: The input items to the model, in OpenAI Responses format.<br>            model_settings: The model settings to use.<br>            tools: The tools available to the model.<br>            output_schema: The output schema to use.<br>            handoffs: The handoffs available to the model.<br>            tracing: Tracing configuration.<br>            previous_response_id: the ID of the previous response. Generally not used by the model,<br>                except for the OpenAI Responses API.<br>            prompt: The prompt config to use for the model.<br>        Returns:<br>            The full model response.<br>        \"\"\"<br>        pass<br>    @abc.abstractmethod<br>    def stream_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchemaBase | None,<br>        handoffs: list[Handoff],<br>        tracing: ModelTracing,<br>        *,<br>        previous_response_id: str | None,<br>        prompt: ResponsePromptParam | None,<br>    ) -> AsyncIterator[TResponseStreamEvent]:<br>        \"\"\"Stream a response from the model.<br>        Args:<br>            system_instructions: The system instructions to use.<br>            input: The input items to the model, in OpenAI Responses format.<br>            model_settings: The model settings to use.<br>            tools: The tools available to the model.<br>            output_schema: The output schema to use.<br>            handoffs: The handoffs available to the model.<br>            tracing: Tracing configuration.<br>            previous_response_id: the ID of the previous response. Generally not used by the model,<br>                except for the OpenAI Responses API.<br>            prompt: The prompt config to use for the model.<br>        Returns:<br>            An iterator of response stream events, in OpenAI Responses format.<br>        \"\"\"<br>        pass<br>``` |\n\n#### get\\_response`abstractmethod``async`\n\n```md-code__content\nget_response(\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    *,\n    previous_response_id: str | None,\n    prompt: ResponsePromptParam | None,\n) -> ModelResponse\n\n```\n\nGet a response from the model.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `system_instructions` | `str | None` | The system instructions to use. | _required_ |\n| `input` | `str | list[TResponseInputItem]` | The input items to the model, in OpenAI Responses format. | _required_ |\n| `model_settings` | `ModelSettings` | The model settings to use. | _required_ |\n| `tools` | `list[Tool]` | The tools available to the model. | _required_ |\n| `output_schema` | `AgentOutputSchemaBase | None` | The output schema to use. | _required_ |\n| `handoffs` | `list[Handoff]` | The handoffs available to the model. | _required_ |\n| `tracing` | `ModelTracing` | Tracing configuration. | _required_ |\n| `previous_response_id` | `str | None` | the ID of the previous response. Generally not used by the model,<br>except for the OpenAI Responses API. | _required_ |\n| `prompt` | `ResponsePromptParam | None` | The prompt config to use for the model. | _required_ |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `ModelResponse` | The full model response. |\n\nSource code in `src/agents/models/interface.py`\n\n|     |     |\n| --- | --- |\n| ```<br>39<br>40<br>41<br>42<br>43<br>44<br>45<br>46<br>47<br>48<br>49<br>50<br>51<br>52<br>53<br>54<br>55<br>56<br>57<br>58<br>59<br>60<br>61<br>62<br>63<br>64<br>65<br>66<br>67<br>68<br>69<br>70<br>``` | ```md-code__content<br>@abc.abstractmethod<br>async def get_response(<br>    self,<br>    system_instructions: str | None,<br>    input: str | list[TResponseInputItem],<br>    model_settings: ModelSettings,<br>    tools: list[Tool],<br>    output_schema: AgentOutputSchemaBase | None,<br>    handoffs: list[Handoff],<br>    tracing: ModelTracing,<br>    *,<br>    previous_response_id: str | None,<br>    prompt: ResponsePromptParam | None,<br>) -> ModelResponse:<br>    \"\"\"Get a response from the model.<br>    Args:<br>        system_instructions: The system instructions to use.<br>        input: The input items to the model, in OpenAI Responses format.<br>        model_settings: The model settings to use.<br>        tools: The tools available to the model.<br>        output_schema: The output schema to use.<br>        handoffs: The handoffs available to the model.<br>        tracing: Tracing configuration.<br>        previous_response_id: the ID of the previous response. Generally not used by the model,<br>            except for the OpenAI Responses API.<br>        prompt: The prompt config to use for the model.<br>    Returns:<br>        The full model response.<br>    \"\"\"<br>    pass<br>``` |\n\n#### stream\\_response`abstractmethod`\n\n```md-code__content\nstream_response(\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    *,\n    previous_response_id: str | None,\n    prompt: ResponsePromptParam | None,\n) -> AsyncIterator[TResponseStreamEvent]\n\n```\n\nStream a response from the model.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `system_instructions` | `str | None` | The system instructions to use. | _required_ |\n| `input` | `str | list[TResponseInputItem]` | The input items to the model, in OpenAI Responses format. | _required_ |\n| `model_settings` | `ModelSettings` | The model settings to use. | _required_ |\n| `tools` | `list[Tool]` | The tools available to the model. | _required_ |\n| `output_schema` | `AgentOutputSchemaBase | None` | The output schema to use. | _required_ |\n| `handoffs` | `list[Handoff]` | The handoffs available to the model. | _required_ |\n| `tracing` | `ModelTracing` | Tracing configuration. | _required_ |\n| `previous_response_id` | `str | None` | the ID of the previous response. Generally not used by the model,<br>except for the OpenAI Responses API. | _required_ |\n| `prompt` | `ResponsePromptParam | None` | The prompt config to use for the model. | _required_ |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[TResponseStreamEvent]` | An iterator of response stream events, in OpenAI Responses format. |\n\nSource code in `src/agents/models/interface.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>``` | ```md-code__content<br>@abc.abstractmethod<br>def stream_response(<br>    self,<br>    system_instructions: str | None,<br>    input: str | list[TResponseInputItem],<br>    model_settings: ModelSettings,<br>    tools: list[Tool],<br>    output_schema: AgentOutputSchemaBase | None,<br>    handoffs: list[Handoff],<br>    tracing: ModelTracing,<br>    *,<br>    previous_response_id: str | None,<br>    prompt: ResponsePromptParam | None,<br>) -> AsyncIterator[TResponseStreamEvent]:<br>    \"\"\"Stream a response from the model.<br>    Args:<br>        system_instructions: The system instructions to use.<br>        input: The input items to the model, in OpenAI Responses format.<br>        model_settings: The model settings to use.<br>        tools: The tools available to the model.<br>        output_schema: The output schema to use.<br>        handoffs: The handoffs available to the model.<br>        tracing: Tracing configuration.<br>        previous_response_id: the ID of the previous response. Generally not used by the model,<br>            except for the OpenAI Responses API.<br>        prompt: The prompt config to use for the model.<br>    Returns:<br>        An iterator of response stream events, in OpenAI Responses format.<br>    \"\"\"<br>    pass<br>``` |\n\n### ModelProvider\n\nBases: `ABC`\n\nThe base interface for a model provider.\n\nModel provider is responsible for looking up Models by name.\n\nSource code in `src/agents/models/interface.py`\n\n|     |     |\n| --- | --- |\n| ```<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>``` | ```md-code__content<br>class ModelProvider(abc.ABC):<br>    \"\"\"The base interface for a model provider.<br>    Model provider is responsible for looking up Models by name.<br>    \"\"\"<br>    @abc.abstractmethod<br>    def get_model(self, model_name: str | None) -> Model:<br>        \"\"\"Get a model by name.<br>        Args:<br>            model_name: The name of the model to get.<br>        Returns:<br>            The model.<br>        \"\"\"<br>``` |\n\n#### get\\_model`abstractmethod`\n\n```md-code__content\nget_model(model_name: str | None) -> Model\n\n```\n\nGet a model by name.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `str | None` | The name of the model to get. | _required_ |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Model` | The model. |\n\nSource code in `src/agents/models/interface.py`\n\n|     |     |\n| --- | --- |\n| ```<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>``` | ```md-code__content<br>@abc.abstractmethod<br>def get_model(self, model_name: str | None) -> Model:<br>    \"\"\"Get a model by name.<br>    Args:<br>        model_name: The name of the model to get.<br>    Returns:<br>        The model.<br>    \"\"\"<br>``` |",
  "metadata": {
    "language": "ja",
    "title": "Model interface - OpenAI Agents SDK",
    "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
    "favicon": "https://openai.github.io/openai-agents-python/images/favicon-platform.svg",
    "viewport": "width=device-width,initial-scale=1",
    "scrapeId": "340f6233-8aad-43a9-9d6f-5ad6c8a49f39",
    "sourceURL": "https://openai.github.io/openai-agents-python/ja/ref/models/interface/",
    "url": "https://openai.github.io/openai-agents-python/ja/ref/models/interface/",
    "statusCode": 200,
    "contentType": "text/html; charset=utf-8",
    "proxyUsed": "basic"
  },
  "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan."
}