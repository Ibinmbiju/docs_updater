{
  "markdown": "[Skip to content](https://openai.github.io/openai-agents-python/voice/quickstart/#quickstart)\n\n# Quickstart\n\n## Prerequisites\n\nMake sure you've followed the base [quickstart instructions](https://openai.github.io/openai-agents-python/quickstart/) for the Agents SDK, and set up a virtual environment. Then, install the optional voice dependencies from the SDK:\n\n```md-code__content\npip install 'openai-agents[voice]'\n\n```\n\n## Concepts\n\nThe main concept to know about is a [`VoicePipeline`](https://openai.github.io/openai-agents-python/ref/voice/pipeline/#agents.voice.pipeline.VoicePipeline \"VoicePipeline\"), which is a 3 step process:\n\n1. Run a speech-to-text model to turn audio into text.\n2. Run your code, which is usually an agentic workflow, to produce a result.\n3. Run a text-to-speech model to turn the result text back into audio.\n\n## Agents\n\nFirst, let's set up some Agents. This should feel familiar to you if you've built any agents with this SDK. We'll have a couple of Agents, a handoff, and a tool.\n\n```md-code__content\nimport asyncio\nimport random\n\nfrom agents import (\n    Agent,\n    function_tool,\n)\nfrom agents.extensions.handoff_prompt import prompt_with_handoff_instructions\n\n@function_tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a given city.\"\"\"\n    print(f\"[debug] get_weather called with city: {city}\")\n    choices = [\"sunny\", \"cloudy\", \"rainy\", \"snowy\"]\n    return f\"The weather in {city} is {random.choice(choices)}.\"\n\nspanish_agent = Agent(\n    name=\"Spanish\",\n    handoff_description=\"A spanish speaking agent.\",\n    instructions=prompt_with_handoff_instructions(\n        \"You're speaking to a human, so be polite and concise. Speak in Spanish.\",\n    ),\n    model=\"gpt-4o-mini\",\n)\n\nagent = Agent(\n    name=\"Assistant\",\n    instructions=prompt_with_handoff_instructions(\n        \"You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.\",\n    ),\n    model=\"gpt-4o-mini\",\n    handoffs=[spanish_agent],\n    tools=[get_weather],\n)\n\n```\n\n## Voice pipeline\n\nWe'll set up a simple voice pipeline, using [`SingleAgentVoiceWorkflow`](https://openai.github.io/openai-agents-python/ref/voice/workflow/#agents.voice.workflow.SingleAgentVoiceWorkflow \"SingleAgentVoiceWorkflow\") as the workflow.\n\n```md-code__content\nfrom agents.voice import SingleAgentVoiceWorkflow, VoicePipeline\npipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))\n\n```\n\n## Run the pipeline\n\n```md-code__content\nimport numpy as np\nimport sounddevice as sd\nfrom agents.voice import AudioInput\n\n# For simplicity, we'll just create 3 seconds of silence\n# In reality, you'd get microphone data\nbuffer = np.zeros(24000 * 3, dtype=np.int16)\naudio_input = AudioInput(buffer=buffer)\n\nresult = await pipeline.run(audio_input)\n\n# Create an audio player using `sounddevice`\nplayer = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)\nplayer.start()\n\n# Play the audio stream as it comes in\nasync for event in result.stream():\n    if event.type == \"voice_stream_event_audio\":\n        player.write(event.data)\n\n```\n\n## Put it all together\n\n```md-code__content\nimport asyncio\nimport random\n\nimport numpy as np\nimport sounddevice as sd\n\nfrom agents import (\n    Agent,\n    function_tool,\n    set_tracing_disabled,\n)\nfrom agents.voice import (\n    AudioInput,\n    SingleAgentVoiceWorkflow,\n    VoicePipeline,\n)\nfrom agents.extensions.handoff_prompt import prompt_with_handoff_instructions\n\n@function_tool\ndef get_weather(city: str) -> str:\n    \"\"\"Get the weather for a given city.\"\"\"\n    print(f\"[debug] get_weather called with city: {city}\")\n    choices = [\"sunny\", \"cloudy\", \"rainy\", \"snowy\"]\n    return f\"The weather in {city} is {random.choice(choices)}.\"\n\nspanish_agent = Agent(\n    name=\"Spanish\",\n    handoff_description=\"A spanish speaking agent.\",\n    instructions=prompt_with_handoff_instructions(\n        \"You're speaking to a human, so be polite and concise. Speak in Spanish.\",\n    ),\n    model=\"gpt-4o-mini\",\n)\n\nagent = Agent(\n    name=\"Assistant\",\n    instructions=prompt_with_handoff_instructions(\n        \"You're speaking to a human, so be polite and concise. If the user speaks in Spanish, handoff to the spanish agent.\",\n    ),\n    model=\"gpt-4o-mini\",\n    handoffs=[spanish_agent],\n    tools=[get_weather],\n)\n\nasync def main():\n    pipeline = VoicePipeline(workflow=SingleAgentVoiceWorkflow(agent))\n    buffer = np.zeros(24000 * 3, dtype=np.int16)\n    audio_input = AudioInput(buffer=buffer)\n\n    result = await pipeline.run(audio_input)\n\n    # Create an audio player using `sounddevice`\n    player = sd.OutputStream(samplerate=24000, channels=1, dtype=np.int16)\n    player.start()\n\n    # Play the audio stream as it comes in\n    async for event in result.stream():\n        if event.type == \"voice_stream_event_audio\":\n            player.write(event.data)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n```\n\nIf you run this example, the agent will speak to you! Check out the example in [examples/voice/static](https://github.com/openai/openai-agents-python/tree/main/examples/voice/static) to see a demo where you can speak to the agent yourself.",
  "metadata": {
    "favicon": "https://openai.github.io/openai-agents-python/images/favicon-platform.svg",
    "title": "Quickstart - OpenAI Agents SDK",
    "viewport": "width=device-width,initial-scale=1",
    "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
    "language": "en",
    "scrapeId": "e6b06901-38c7-43df-83e6-5cd900403260",
    "sourceURL": "https://openai.github.io/openai-agents-python/voice/quickstart/",
    "url": "https://openai.github.io/openai-agents-python/voice/quickstart/",
    "statusCode": 200,
    "contentType": "text/html; charset=utf-8",
    "proxyUsed": "basic"
  }
}