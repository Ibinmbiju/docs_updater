{
  "markdown": "[Skip to content](https://openai.github.io/openai-agents-python/tools/#tools)\n\n# Tools\n\nTools let agents take actions: things like fetching data, running code, calling external APIs, and even using a computer. There are three classes of tools in the Agent SDK:\n\n- Hosted tools: these run on LLM servers alongside the AI models. OpenAI offers retrieval, web search and computer use as hosted tools.\n- Function calling: these allow you to use any Python function as a tool.\n- Agents as tools: this allows you to use an agent as a tool, allowing Agents to call other agents without handing off to them.\n\n## Hosted tools\n\nOpenAI offers a few built-in tools when using the [`OpenAIResponsesModel`](https://openai.github.io/openai-agents-python/ref/models/openai_responses/#agents.models.openai_responses.OpenAIResponsesModel \"OpenAIResponsesModel\"):\n\n- The [`WebSearchTool`](https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.WebSearchTool \"WebSearchTool            dataclass   \") lets an agent search the web.\n- The [`FileSearchTool`](https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.FileSearchTool \"FileSearchTool            dataclass   \") allows retrieving information from your OpenAI Vector Stores.\n- The [`ComputerTool`](https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.ComputerTool \"ComputerTool            dataclass   \") allows automating computer use tasks.\n- The [`CodeInterpreterTool`](https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.CodeInterpreterTool \"CodeInterpreterTool            dataclass   \") lets the LLM execute code in a sandboxed environment.\n- The [`HostedMCPTool`](https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.HostedMCPTool \"HostedMCPTool            dataclass   \") exposes a remote MCP server's tools to the model.\n- The [`ImageGenerationTool`](https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.ImageGenerationTool \"ImageGenerationTool            dataclass   \") generates images from a prompt.\n- The [`LocalShellTool`](https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.LocalShellTool \"LocalShellTool            dataclass   \") runs shell commands on your machine.\n\n```md-code__content\nfrom agents import Agent, FileSearchTool, Runner, WebSearchTool\n\nagent = Agent(\n    name=\"Assistant\",\n    tools=[\\\n        WebSearchTool(),\\\n        FileSearchTool(\\\n            max_num_results=3,\\\n            vector_store_ids=[\"VECTOR_STORE_ID\"],\\\n        ),\\\n    ],\n)\n\nasync def main():\n    result = await Runner.run(agent, \"Which coffee shop should I go to, taking into account my preferences and the weather today in SF?\")\n    print(result.final_output)\n\n```\n\n## Function tools\n\nYou can use any Python function as a tool. The Agents SDK will setup the tool automatically:\n\n- The name of the tool will be the name of the Python function (or you can provide a name)\n- Tool description will be taken from the docstring of the function (or you can provide a description)\n- The schema for the function inputs is automatically created from the function's arguments\n- Descriptions for each input are taken from the docstring of the function, unless disabled\n\nWe use Python's `inspect` module to extract the function signature, along with [`griffe`](https://mkdocstrings.github.io/griffe/) to parse docstrings and `pydantic` for schema creation.\n\n```md-code__content\nimport json\n\nfrom typing_extensions import TypedDict, Any\n\nfrom agents import Agent, FunctionTool, RunContextWrapper, function_tool\n\nclass Location(TypedDict):\n    lat: float\n    long: float\n\n@function_tool\nasync def fetch_weather(location: Location) -> str:\n\n    \"\"\"Fetch the weather for a given location.\n\n    Args:\n        location: The location to fetch the weather for.\n    \"\"\"\n    # In real life, we'd fetch the weather from a weather API\n    return \"sunny\"\n\n@function_tool(name_override=\"fetch_data\")\ndef read_file(ctx: RunContextWrapper[Any], path: str, directory: str | None = None) -> str:\n    \"\"\"Read the contents of a file.\n\n    Args:\n        path: The path to the file to read.\n        directory: The directory to read the file from.\n    \"\"\"\n    # In real life, we'd read the file from the file system\n    return \"<file contents>\"\n\nagent = Agent(\n    name=\"Assistant\",\n    tools=[fetch_weather, read_file],\n)\n\nfor tool in agent.tools:\n    if isinstance(tool, FunctionTool):\n        print(tool.name)\n        print(tool.description)\n        print(json.dumps(tool.params_json_schema, indent=2))\n        print()\n\n```\n\nExpand to see output\n\n```md-code__content\nfetch_weather\nFetch the weather for a given location.\n{\n\"$defs\": {\n  \"Location\": {\n    \"properties\": {\n      \"lat\": {\n        \"title\": \"Lat\",\n        \"type\": \"number\"\n      },\n      \"long\": {\n        \"title\": \"Long\",\n        \"type\": \"number\"\n      }\n    },\n    \"required\": [\\\n      \"lat\",\\\n      \"long\"\\\n    ],\n    \"title\": \"Location\",\n    \"type\": \"object\"\n  }\n},\n\"properties\": {\n  \"location\": {\n    \"$ref\": \"#/$defs/Location\",\n    \"description\": \"The location to fetch the weather for.\"\n  }\n},\n\"required\": [\\\n  \"location\"\\\n],\n\"title\": \"fetch_weather_args\",\n\"type\": \"object\"\n}\n\nfetch_data\nRead the contents of a file.\n{\n\"properties\": {\n  \"path\": {\n    \"description\": \"The path to the file to read.\",\n    \"title\": \"Path\",\n    \"type\": \"string\"\n  },\n  \"directory\": {\n    \"anyOf\": [\\\n      {\\\n        \"type\": \"string\"\\\n      },\\\n      {\\\n        \"type\": \"null\"\\\n      }\\\n    ],\n    \"default\": null,\n    \"description\": \"The directory to read the file from.\",\n    \"title\": \"Directory\"\n  }\n},\n\"required\": [\\\n  \"path\"\\\n],\n\"title\": \"fetch_data_args\",\n\"type\": \"object\"\n}\n\n```\n\n### Custom function tools\n\nSometimes, you don't want to use a Python function as a tool. You can directly create a [`FunctionTool`](https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.FunctionTool \"FunctionTool            dataclass   \") if you prefer. You'll need to provide:\n\n- `name`\n- `description`\n- `params_json_schema`, which is the JSON schema for the arguments\n- `on_invoke_tool`, which is an async function that receives the context and the arguments as a JSON string, and must return the tool output as a string.\n\n```md-code__content\nfrom typing import Any\n\nfrom pydantic import BaseModel\n\nfrom agents import RunContextWrapper, FunctionTool\n\ndef do_some_work(data: str) -> str:\n    return \"done\"\n\nclass FunctionArgs(BaseModel):\n    username: str\n    age: int\n\nasync def run_function(ctx: RunContextWrapper[Any], args: str) -> str:\n    parsed = FunctionArgs.model_validate_json(args)\n    return do_some_work(data=f\"{parsed.username} is {parsed.age} years old\")\n\ntool = FunctionTool(\n    name=\"process_user\",\n    description=\"Processes extracted user data\",\n    params_json_schema=FunctionArgs.model_json_schema(),\n    on_invoke_tool=run_function,\n)\n\n```\n\n### Automatic argument and docstring parsing\n\nAs mentioned before, we automatically parse the function signature to extract the schema for the tool, and we parse the docstring to extract descriptions for the tool and for individual arguments. Some notes on that:\n\n1. The signature parsing is done via the `inspect` module. We use type annotations to understand the types for the arguments, and dynamically build a Pydantic model to represent the overall schema. It supports most types, including Python primitives, Pydantic models, TypedDicts, and more.\n2. We use `griffe` to parse docstrings. Supported docstring formats are `google`, `sphinx` and `numpy`. We attempt to automatically detect the docstring format, but this is best-effort and you can explicitly set it when calling `function_tool`. You can also disable docstring parsing by setting `use_docstring_info` to `False`.\n\nThe code for the schema extraction lives in [`agents.function_schema`](https://openai.github.io/openai-agents-python/ref/function_schema/#agents.function_schema).\n\n## Agents as tools\n\nIn some workflows, you may want a central agent to orchestrate a network of specialized agents, instead of handing off control. You can do this by modeling agents as tools.\n\n```md-code__content\nfrom agents import Agent, Runner\nimport asyncio\n\nspanish_agent = Agent(\n    name=\"Spanish agent\",\n    instructions=\"You translate the user's message to Spanish\",\n)\n\nfrench_agent = Agent(\n    name=\"French agent\",\n    instructions=\"You translate the user's message to French\",\n)\n\norchestrator_agent = Agent(\n    name=\"orchestrator_agent\",\n    instructions=(\n        \"You are a translation agent. You use the tools given to you to translate.\"\n        \"If asked for multiple translations, you call the relevant tools.\"\n    ),\n    tools=[\\\n        spanish_agent.as_tool(\\\n            tool_name=\"translate_to_spanish\",\\\n            tool_description=\"Translate the user's message to Spanish\",\\\n        ),\\\n        french_agent.as_tool(\\\n            tool_name=\"translate_to_french\",\\\n            tool_description=\"Translate the user's message to French\",\\\n        ),\\\n    ],\n)\n\nasync def main():\n    result = await Runner.run(orchestrator_agent, input=\"Say 'Hello, how are you?' in Spanish.\")\n    print(result.final_output)\n\n```\n\n### Customizing tool-agents\n\nThe `agent.as_tool` function is a convenience method to make it easy to turn an agent into a tool. It doesn't support all configuration though; for example, you can't set `max_turns`. For advanced use cases, use `Runner.run` directly in your tool implementation:\n\n```md-code__content\n@function_tool\nasync def run_my_agent() -> str:\n    \"\"\"A tool that runs the agent with custom configs\"\"\"\n\n    agent = Agent(name=\"My agent\", instructions=\"...\")\n\n    result = await Runner.run(\n        agent,\n        input=\"...\",\n        max_turns=5,\n        run_config=...\n    )\n\n    return str(result.final_output)\n\n```\n\n### Custom output extraction\n\nIn certain cases, you might want to modify the output of the tool-agents before returning it to the central agent. This may be useful if you want to:\n\n- Extract a specific piece of information (e.g., a JSON payload) from the sub-agent's chat history.\n- Convert or reformat the agent’s final answer (e.g., transform Markdown into plain text or CSV).\n- Validate the output or provide a fallback value when the agent’s response is missing or malformed.\n\nYou can do this by supplying the `custom_output_extractor` argument to the `as_tool` method:\n\n```md-code__content\nasync def extract_json_payload(run_result: RunResult) -> str:\n    # Scan the agent’s outputs in reverse order until we find a JSON-like message from a tool call.\n    for item in reversed(run_result.new_items):\n        if isinstance(item, ToolCallOutputItem) and item.output.strip().startswith(\"{\"):\n            return item.output.strip()\n    # Fallback to an empty JSON object if nothing was found\n    return \"{}\"\n\njson_tool = data_agent.as_tool(\n    tool_name=\"get_data_json\",\n    tool_description=\"Run the data agent and return only its JSON payload\",\n    custom_output_extractor=extract_json_payload,\n)\n\n```\n\n## Handling errors in function tools\n\nWhen you create a function tool via `@function_tool`, you can pass a `failure_error_function`. This is a function that provides an error response to the LLM in case the tool call crashes.\n\n- By default (i.e. if you don't pass anything), it runs a `default_tool_error_function` which tells the LLM an error occurred.\n- If you pass your own error function, it runs that instead, and sends the response to the LLM.\n- If you explicitly pass `None`, then any tool call errors will be re-raised for you to handle. This could be a `ModelBehaviorError` if the model produced invalid JSON, or a `UserError` if your code crashed, etc.\n\nIf you are manually creating a `FunctionTool` object, then you must handle errors inside the `on_invoke_tool` function.",
  "metadata": {
    "viewport": "width=device-width,initial-scale=1",
    "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
    "title": "Tools - OpenAI Agents SDK",
    "favicon": "https://openai.github.io/openai-agents-python/images/favicon-platform.svg",
    "language": "en",
    "scrapeId": "4f2fb567-e4a0-4db9-867c-e181960f958d",
    "sourceURL": "https://openai.github.io/openai-agents-python/tools/",
    "url": "https://openai.github.io/openai-agents-python/tools/",
    "statusCode": 200,
    "contentType": "text/html; charset=utf-8",
    "proxyUsed": "basic"
  }
}