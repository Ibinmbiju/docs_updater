{
  "markdown": "[Skip to content](https://openai.github.io/openai-agents-python/ref/extensions/litellm/#litellm-models)\n\n# `LiteLLM Models`\n\n### LitellmModel\n\nBases: `Model`\n\nThis class enables using any model via LiteLLM. LiteLLM allows you to acess OpenAPI,\nAnthropic, Gemini, Mistral, and many other models.\nSee supported models here: [litellm models](https://docs.litellm.ai/docs/providers).\n\nSource code in `src/agents/extensions/models/litellm_model.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>318<br>319<br>320<br>321<br>322<br>323<br>324<br>325<br>326<br>327<br>328<br>329<br>330<br>331<br>332<br>333<br>334<br>335<br>336<br>337<br>338<br>339<br>340<br>341<br>``` | ```md-code__content<br>class LitellmModel(Model):<br>    \"\"\"This class enables using any model via LiteLLM. LiteLLM allows you to acess OpenAPI,<br>    Anthropic, Gemini, Mistral, and many other models.<br>    See supported models here: [litellm models](https://docs.litellm.ai/docs/providers).<br>    \"\"\"<br>    def __init__(<br>        self,<br>        model: str,<br>        base_url: str | None = None,<br>        api_key: str | None = None,<br>    ):<br>        self.model = model<br>        self.base_url = base_url<br>        self.api_key = api_key<br>    async def get_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchemaBase | None,<br>        handoffs: list[Handoff],<br>        tracing: ModelTracing,<br>        previous_response_id: str | None,<br>        prompt: Any | None = None,<br>    ) -> ModelResponse:<br>        with generation_span(<br>            model=str(self.model),<br>            model_config=model_settings.to_json_dict()<br>            | {\"base_url\": str(self.base_url or \"\"), \"model_impl\": \"litellm\"},<br>            disabled=tracing.is_disabled(),<br>        ) as span_generation:<br>            response = await self._fetch_response(<br>                system_instructions,<br>                input,<br>                model_settings,<br>                tools,<br>                output_schema,<br>                handoffs,<br>                span_generation,<br>                tracing,<br>                stream=False,<br>                prompt=prompt,<br>            )<br>            assert isinstance(response.choices[0], litellm.types.utils.Choices)<br>            if _debug.DONT_LOG_MODEL_DATA:<br>                logger.debug(\"Received model response\")<br>            else:<br>                logger.debug(<br>                    f\"LLM resp:\\n{json.dumps(response.choices[0].message.model_dump(), indent=2)}\\n\"<br>                )<br>            if hasattr(response, \"usage\"):<br>                response_usage = response.usage<br>                usage = (<br>                    Usage(<br>                        requests=1,<br>                        input_tokens=response_usage.prompt_tokens,<br>                        output_tokens=response_usage.completion_tokens,<br>                        total_tokens=response_usage.total_tokens,<br>                        input_tokens_details=InputTokensDetails(<br>                            cached_tokens=getattr(<br>                                response_usage.prompt_tokens_details, \"cached_tokens\", 0<br>                            )<br>                            or 0<br>                        ),<br>                        output_tokens_details=OutputTokensDetails(<br>                            reasoning_tokens=getattr(<br>                                response_usage.completion_tokens_details, \"reasoning_tokens\", 0<br>                            )<br>                            or 0<br>                        ),<br>                    )<br>                    if response.usage<br>                    else Usage()<br>                )<br>            else:<br>                usage = Usage()<br>                logger.warning(\"No usage information returned from Litellm\")<br>            if tracing.include_data():<br>                span_generation.span_data.output = [response.choices[0].message.model_dump()]<br>            span_generation.span_data.usage = {<br>                \"input_tokens\": usage.input_tokens,<br>                \"output_tokens\": usage.output_tokens,<br>            }<br>            items = Converter.message_to_output_items(<br>                LitellmConverter.convert_message_to_openai(response.choices[0].message)<br>            )<br>            return ModelResponse(<br>                output=items,<br>                usage=usage,<br>                response_id=None,<br>            )<br>    async def stream_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchemaBase | None,<br>        handoffs: list[Handoff],<br>        tracing: ModelTracing,<br>        previous_response_id: str | None,<br>        prompt: Any | None = None,<br>    ) -> AsyncIterator[TResponseStreamEvent]:<br>        with generation_span(<br>            model=str(self.model),<br>            model_config=model_settings.to_json_dict()<br>            | {\"base_url\": str(self.base_url or \"\"), \"model_impl\": \"litellm\"},<br>            disabled=tracing.is_disabled(),<br>        ) as span_generation:<br>            response, stream = await self._fetch_response(<br>                system_instructions,<br>                input,<br>                model_settings,<br>                tools,<br>                output_schema,<br>                handoffs,<br>                span_generation,<br>                tracing,<br>                stream=True,<br>                prompt=prompt,<br>            )<br>            final_response: Response | None = None<br>            async for chunk in ChatCmplStreamHandler.handle_stream(response, stream):<br>                yield chunk<br>                if chunk.type == \"response.completed\":<br>                    final_response = chunk.response<br>            if tracing.include_data() and final_response:<br>                span_generation.span_data.output = [final_response.model_dump()]<br>            if final_response and final_response.usage:<br>                span_generation.span_data.usage = {<br>                    \"input_tokens\": final_response.usage.input_tokens,<br>                    \"output_tokens\": final_response.usage.output_tokens,<br>                }<br>    @overload<br>    async def _fetch_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchemaBase | None,<br>        handoffs: list[Handoff],<br>        span: Span[GenerationSpanData],<br>        tracing: ModelTracing,<br>        stream: Literal[True],<br>        prompt: Any | None = None,<br>    ) -> tuple[Response, AsyncStream[ChatCompletionChunk]]: ...<br>    @overload<br>    async def _fetch_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchemaBase | None,<br>        handoffs: list[Handoff],<br>        span: Span[GenerationSpanData],<br>        tracing: ModelTracing,<br>        stream: Literal[False],<br>        prompt: Any | None = None,<br>    ) -> litellm.types.utils.ModelResponse: ...<br>    async def _fetch_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchemaBase | None,<br>        handoffs: list[Handoff],<br>        span: Span[GenerationSpanData],<br>        tracing: ModelTracing,<br>        stream: bool = False,<br>        prompt: Any | None = None,<br>    ) -> litellm.types.utils.ModelResponse | tuple[Response, AsyncStream[ChatCompletionChunk]]:<br>        converted_messages = Converter.items_to_messages(input)<br>        if system_instructions:<br>            converted_messages.insert(<br>                0,<br>                {<br>                    \"content\": system_instructions,<br>                    \"role\": \"system\",<br>                },<br>            )<br>        if tracing.include_data():<br>            span.span_data.input = converted_messages<br>        parallel_tool_calls = (<br>            True<br>            if model_settings.parallel_tool_calls and tools and len(tools) > 0<br>            else False<br>            if model_settings.parallel_tool_calls is False<br>            else None<br>        )<br>        tool_choice = Converter.convert_tool_choice(model_settings.tool_choice)<br>        response_format = Converter.convert_response_format(output_schema)<br>        converted_tools = [Converter.tool_to_openai(tool) for tool in tools] if tools else []<br>        for handoff in handoffs:<br>            converted_tools.append(Converter.convert_handoff_tool(handoff))<br>        if _debug.DONT_LOG_MODEL_DATA:<br>            logger.debug(\"Calling LLM\")<br>        else:<br>            logger.debug(<br>                f\"Calling Litellm model: {self.model}\\n\"<br>                f\"{json.dumps(converted_messages, indent=2)}\\n\"<br>                f\"Tools:\\n{json.dumps(converted_tools, indent=2)}\\n\"<br>                f\"Stream: {stream}\\n\"<br>                f\"Tool choice: {tool_choice}\\n\"<br>                f\"Response format: {response_format}\\n\"<br>            )<br>        reasoning_effort = model_settings.reasoning.effort if model_settings.reasoning else None<br>        stream_options = None<br>        if stream and model_settings.include_usage is not None:<br>            stream_options = {\"include_usage\": model_settings.include_usage}<br>        extra_kwargs = {}<br>        if model_settings.extra_query:<br>            extra_kwargs[\"extra_query\"] = model_settings.extra_query<br>        if model_settings.metadata:<br>            extra_kwargs[\"metadata\"] = model_settings.metadata<br>        if model_settings.extra_body and isinstance(model_settings.extra_body, dict):<br>            extra_kwargs.update(model_settings.extra_body)<br>        # Add kwargs from model_settings.extra_args, filtering out None values<br>        if model_settings.extra_args:<br>            extra_kwargs.update(model_settings.extra_args)<br>        ret = await litellm.acompletion(<br>            model=self.model,<br>            messages=converted_messages,<br>            tools=converted_tools or None,<br>            temperature=model_settings.temperature,<br>            top_p=model_settings.top_p,<br>            frequency_penalty=model_settings.frequency_penalty,<br>            presence_penalty=model_settings.presence_penalty,<br>            max_tokens=model_settings.max_tokens,<br>            tool_choice=self._remove_not_given(tool_choice),<br>            response_format=self._remove_not_given(response_format),<br>            parallel_tool_calls=parallel_tool_calls,<br>            stream=stream,<br>            stream_options=stream_options,<br>            reasoning_effort=reasoning_effort,<br>            extra_headers={**HEADERS, **(model_settings.extra_headers or {})},<br>            api_key=self.api_key,<br>            base_url=self.base_url,<br>            **extra_kwargs,<br>        )<br>        if isinstance(ret, litellm.types.utils.ModelResponse):<br>            return ret<br>        response = Response(<br>            id=FAKE_RESPONSES_ID,<br>            created_at=time.time(),<br>            model=self.model,<br>            object=\"response\",<br>            output=[],<br>            tool_choice=cast(Literal[\"auto\", \"required\", \"none\"], tool_choice)<br>            if tool_choice != NOT_GIVEN<br>            else \"auto\",<br>            top_p=model_settings.top_p,<br>            temperature=model_settings.temperature,<br>            tools=[],<br>            parallel_tool_calls=parallel_tool_calls or False,<br>            reasoning=model_settings.reasoning,<br>        )<br>        return response, ret<br>    def _remove_not_given(self, value: Any) -> Any:<br>        if isinstance(value, NotGiven):<br>            return None<br>        return value<br>``` |",
  "metadata": {
    "language": "en",
    "title": "LiteLLM Models - OpenAI Agents SDK",
    "favicon": "https://openai.github.io/openai-agents-python/images/favicon-platform.svg",
    "viewport": "width=device-width,initial-scale=1",
    "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
    "scrapeId": "da7df41e-69c7-492e-80d4-0301e861ce5c",
    "sourceURL": "https://openai.github.io/openai-agents-python/ref/extensions/litellm/",
    "url": "https://openai.github.io/openai-agents-python/ref/extensions/litellm/",
    "statusCode": 200,
    "contentType": "text/html; charset=utf-8",
    "proxyUsed": "basic"
  }
}