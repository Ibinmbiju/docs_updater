{
  "markdown": "[Skip to content](https://openai.github.io/openai-agents-python/ref/models/openai_chatcompletions/#openai-chat-completions-model)\n\n# `OpenAI Chat Completions model`\n\n### OpenAIChatCompletionsModel\n\nBases: `Model`\n\nSource code in `src/agents/models/openai_chatcompletions.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 35<br> 36<br> 37<br> 38<br> 39<br> 40<br> 41<br> 42<br> 43<br> 44<br> 45<br> 46<br> 47<br> 48<br> 49<br> 50<br> 51<br> 52<br> 53<br> 54<br> 55<br> 56<br> 57<br> 58<br> 59<br> 60<br> 61<br> 62<br> 63<br> 64<br> 65<br> 66<br> 67<br> 68<br> 69<br> 70<br> 71<br> 72<br> 73<br> 74<br> 75<br> 76<br> 77<br> 78<br> 79<br> 80<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>308<br>309<br>310<br>311<br>312<br>313<br>314<br>315<br>316<br>317<br>``` | ```md-code__content<br>class OpenAIChatCompletionsModel(Model):<br>    def __init__(<br>        self,<br>        model: str | ChatModel,<br>        openai_client: AsyncOpenAI,<br>    ) -> None:<br>        self.model = model<br>        self._client = openai_client<br>    def _non_null_or_not_given(self, value: Any) -> Any:<br>        return value if value is not None else NOT_GIVEN<br>    async def get_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchemaBase | None,<br>        handoffs: list[Handoff],<br>        tracing: ModelTracing,<br>        previous_response_id: str | None,<br>        prompt: ResponsePromptParam | None = None,<br>    ) -> ModelResponse:<br>        with generation_span(<br>            model=str(self.model),<br>            model_config=model_settings.to_json_dict() | {\"base_url\": str(self._client.base_url)},<br>            disabled=tracing.is_disabled(),<br>        ) as span_generation:<br>            response = await self._fetch_response(<br>                system_instructions,<br>                input,<br>                model_settings,<br>                tools,<br>                output_schema,<br>                handoffs,<br>                span_generation,<br>                tracing,<br>                stream=False,<br>                prompt=prompt,<br>            )<br>            first_choice = response.choices[0]<br>            message = first_choice.message<br>            if _debug.DONT_LOG_MODEL_DATA:<br>                logger.debug(\"Received model response\")<br>            else:<br>                if message is not None:<br>                    logger.debug(<br>                        \"LLM resp:\\n%s\\n\",<br>                        json.dumps(message.model_dump(), indent=2),<br>                    )<br>                else:<br>                    logger.debug(<br>                        \"LLM resp had no message. finish_reason: %s\",<br>                        first_choice.finish_reason,<br>                    )<br>            usage = (<br>                Usage(<br>                    requests=1,<br>                    input_tokens=response.usage.prompt_tokens,<br>                    output_tokens=response.usage.completion_tokens,<br>                    total_tokens=response.usage.total_tokens,<br>                    input_tokens_details=InputTokensDetails(<br>                        cached_tokens=getattr(<br>                            response.usage.prompt_tokens_details, \"cached_tokens\", 0<br>                        )<br>                        or 0,<br>                    ),<br>                    output_tokens_details=OutputTokensDetails(<br>                        reasoning_tokens=getattr(<br>                            response.usage.completion_tokens_details, \"reasoning_tokens\", 0<br>                        )<br>                        or 0,<br>                    ),<br>                )<br>                if response.usage<br>                else Usage()<br>            )<br>            if tracing.include_data():<br>                span_generation.span_data.output = (<br>                    [message.model_dump()] if message is not None else []<br>                )<br>            span_generation.span_data.usage = {<br>                \"input_tokens\": usage.input_tokens,<br>                \"output_tokens\": usage.output_tokens,<br>            }<br>            items = Converter.message_to_output_items(message) if message is not None else []<br>            return ModelResponse(<br>                output=items,<br>                usage=usage,<br>                response_id=None,<br>            )<br>    async def stream_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchemaBase | None,<br>        handoffs: list[Handoff],<br>        tracing: ModelTracing,<br>        previous_response_id: str | None,<br>        prompt: ResponsePromptParam | None = None,<br>    ) -> AsyncIterator[TResponseStreamEvent]:<br>        \"\"\"<br>        Yields a partial message as it is generated, as well as the usage information.<br>        \"\"\"<br>        with generation_span(<br>            model=str(self.model),<br>            model_config=model_settings.to_json_dict() | {\"base_url\": str(self._client.base_url)},<br>            disabled=tracing.is_disabled(),<br>        ) as span_generation:<br>            response, stream = await self._fetch_response(<br>                system_instructions,<br>                input,<br>                model_settings,<br>                tools,<br>                output_schema,<br>                handoffs,<br>                span_generation,<br>                tracing,<br>                stream=True,<br>                prompt=prompt,<br>            )<br>            final_response: Response | None = None<br>            async for chunk in ChatCmplStreamHandler.handle_stream(response, stream):<br>                yield chunk<br>                if chunk.type == \"response.completed\":<br>                    final_response = chunk.response<br>            if tracing.include_data() and final_response:<br>                span_generation.span_data.output = [final_response.model_dump()]<br>            if final_response and final_response.usage:<br>                span_generation.span_data.usage = {<br>                    \"input_tokens\": final_response.usage.input_tokens,<br>                    \"output_tokens\": final_response.usage.output_tokens,<br>                }<br>    @overload<br>    async def _fetch_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchemaBase | None,<br>        handoffs: list[Handoff],<br>        span: Span[GenerationSpanData],<br>        tracing: ModelTracing,<br>        stream: Literal[True],<br>        prompt: ResponsePromptParam | None = None,<br>    ) -> tuple[Response, AsyncStream[ChatCompletionChunk]]: ...<br>    @overload<br>    async def _fetch_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchemaBase | None,<br>        handoffs: list[Handoff],<br>        span: Span[GenerationSpanData],<br>        tracing: ModelTracing,<br>        stream: Literal[False],<br>        prompt: ResponsePromptParam | None = None,<br>    ) -> ChatCompletion: ...<br>    async def _fetch_response(<br>        self,<br>        system_instructions: str | None,<br>        input: str | list[TResponseInputItem],<br>        model_settings: ModelSettings,<br>        tools: list[Tool],<br>        output_schema: AgentOutputSchemaBase | None,<br>        handoffs: list[Handoff],<br>        span: Span[GenerationSpanData],<br>        tracing: ModelTracing,<br>        stream: bool = False,<br>        prompt: ResponsePromptParam | None = None,<br>    ) -> ChatCompletion | tuple[Response, AsyncStream[ChatCompletionChunk]]:<br>        converted_messages = Converter.items_to_messages(input)<br>        if system_instructions:<br>            converted_messages.insert(<br>                0,<br>                {<br>                    \"content\": system_instructions,<br>                    \"role\": \"system\",<br>                },<br>            )<br>        if tracing.include_data():<br>            span.span_data.input = converted_messages<br>        parallel_tool_calls = (<br>            True<br>            if model_settings.parallel_tool_calls and tools and len(tools) > 0<br>            else False<br>            if model_settings.parallel_tool_calls is False<br>            else NOT_GIVEN<br>        )<br>        tool_choice = Converter.convert_tool_choice(model_settings.tool_choice)<br>        response_format = Converter.convert_response_format(output_schema)<br>        converted_tools = [Converter.tool_to_openai(tool) for tool in tools] if tools else []<br>        for handoff in handoffs:<br>            converted_tools.append(Converter.convert_handoff_tool(handoff))<br>        if _debug.DONT_LOG_MODEL_DATA:<br>            logger.debug(\"Calling LLM\")<br>        else:<br>            logger.debug(<br>                f\"{json.dumps(converted_messages, indent=2)}\\n\"<br>                f\"Tools:\\n{json.dumps(converted_tools, indent=2)}\\n\"<br>                f\"Stream: {stream}\\n\"<br>                f\"Tool choice: {tool_choice}\\n\"<br>                f\"Response format: {response_format}\\n\"<br>            )<br>        reasoning_effort = model_settings.reasoning.effort if model_settings.reasoning else None<br>        store = ChatCmplHelpers.get_store_param(self._get_client(), model_settings)<br>        stream_options = ChatCmplHelpers.get_stream_options_param(<br>            self._get_client(), model_settings, stream=stream<br>        )<br>        ret = await self._get_client().chat.completions.create(<br>            model=self.model,<br>            messages=converted_messages,<br>            tools=converted_tools or NOT_GIVEN,<br>            temperature=self._non_null_or_not_given(model_settings.temperature),<br>            top_p=self._non_null_or_not_given(model_settings.top_p),<br>            frequency_penalty=self._non_null_or_not_given(model_settings.frequency_penalty),<br>            presence_penalty=self._non_null_or_not_given(model_settings.presence_penalty),<br>            max_tokens=self._non_null_or_not_given(model_settings.max_tokens),<br>            tool_choice=tool_choice,<br>            response_format=response_format,<br>            parallel_tool_calls=parallel_tool_calls,<br>            stream=stream,<br>            stream_options=self._non_null_or_not_given(stream_options),<br>            store=self._non_null_or_not_given(store),<br>            reasoning_effort=self._non_null_or_not_given(reasoning_effort),<br>            extra_headers={**HEADERS, **(model_settings.extra_headers or {})},<br>            extra_query=model_settings.extra_query,<br>            extra_body=model_settings.extra_body,<br>            metadata=self._non_null_or_not_given(model_settings.metadata),<br>            **(model_settings.extra_args or {}),<br>        )<br>        if isinstance(ret, ChatCompletion):<br>            return ret<br>        response = Response(<br>            id=FAKE_RESPONSES_ID,<br>            created_at=time.time(),<br>            model=self.model,<br>            object=\"response\",<br>            output=[],<br>            tool_choice=cast(Literal[\"auto\", \"required\", \"none\"], tool_choice)<br>            if tool_choice != NOT_GIVEN<br>            else \"auto\",<br>            top_p=model_settings.top_p,<br>            temperature=model_settings.temperature,<br>            tools=[],<br>            parallel_tool_calls=parallel_tool_calls or False,<br>            reasoning=model_settings.reasoning,<br>        )<br>        return response, ret<br>    def _get_client(self) -> AsyncOpenAI:<br>        if self._client is None:<br>            self._client = AsyncOpenAI()<br>        return self._client<br>``` |\n\n#### stream\\_response`async`\n\n```md-code__content\nstream_response(\n    system_instructions: str | None,\n    input: str | list[TResponseInputItem],\n    model_settings: ModelSettings,\n    tools: list[Tool],\n    output_schema: AgentOutputSchemaBase | None,\n    handoffs: list[Handoff],\n    tracing: ModelTracing,\n    previous_response_id: str | None,\n    prompt: ResponsePromptParam | None = None,\n) -> AsyncIterator[TResponseStreamEvent]\n\n```\n\nYields a partial message as it is generated, as well as the usage information.\n\nSource code in `src/agents/models/openai_chatcompletions.py`\n\n|     |     |\n| --- | --- |\n| ```<br>133<br>134<br>135<br>136<br>137<br>138<br>139<br>140<br>141<br>142<br>143<br>144<br>145<br>146<br>147<br>148<br>149<br>150<br>151<br>152<br>153<br>154<br>155<br>156<br>157<br>158<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>``` | ```md-code__content<br>async def stream_response(<br>    self,<br>    system_instructions: str | None,<br>    input: str | list[TResponseInputItem],<br>    model_settings: ModelSettings,<br>    tools: list[Tool],<br>    output_schema: AgentOutputSchemaBase | None,<br>    handoffs: list[Handoff],<br>    tracing: ModelTracing,<br>    previous_response_id: str | None,<br>    prompt: ResponsePromptParam | None = None,<br>) -> AsyncIterator[TResponseStreamEvent]:<br>    \"\"\"<br>    Yields a partial message as it is generated, as well as the usage information.<br>    \"\"\"<br>    with generation_span(<br>        model=str(self.model),<br>        model_config=model_settings.to_json_dict() | {\"base_url\": str(self._client.base_url)},<br>        disabled=tracing.is_disabled(),<br>    ) as span_generation:<br>        response, stream = await self._fetch_response(<br>            system_instructions,<br>            input,<br>            model_settings,<br>            tools,<br>            output_schema,<br>            handoffs,<br>            span_generation,<br>            tracing,<br>            stream=True,<br>            prompt=prompt,<br>        )<br>        final_response: Response | None = None<br>        async for chunk in ChatCmplStreamHandler.handle_stream(response, stream):<br>            yield chunk<br>            if chunk.type == \"response.completed\":<br>                final_response = chunk.response<br>        if tracing.include_data() and final_response:<br>            span_generation.span_data.output = [final_response.model_dump()]<br>        if final_response and final_response.usage:<br>            span_generation.span_data.usage = {<br>                \"input_tokens\": final_response.usage.input_tokens,<br>                \"output_tokens\": final_response.usage.output_tokens,<br>            }<br>``` |",
  "metadata": {
    "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
    "viewport": "width=device-width,initial-scale=1",
    "title": "OpenAI Chat Completions model - OpenAI Agents SDK",
    "favicon": "https://openai.github.io/openai-agents-python/images/favicon-platform.svg",
    "language": "en",
    "scrapeId": "d36c052d-fb9e-470d-bd1d-a692d6bf515e",
    "sourceURL": "https://openai.github.io/openai-agents-python/ref/models/openai_chatcompletions/",
    "url": "https://openai.github.io/openai-agents-python/ref/models/openai_chatcompletions/",
    "statusCode": 200,
    "contentType": "text/html; charset=utf-8",
    "proxyUsed": "basic"
  }
}