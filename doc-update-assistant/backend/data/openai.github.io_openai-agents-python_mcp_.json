{
  "markdown": "[Skip to content](https://openai.github.io/openai-agents-python/mcp/#model-context-protocol-mcp)\n\n# Model context protocol (MCP)\n\nThe [Model context protocol](https://modelcontextprotocol.io/introduction) (aka MCP) is a way to provide tools and context to the LLM. From the MCP docs:\n\n> MCP is an open protocol that standardizes how applications provide context to LLMs. Think of MCP like a USB-C port for AI applications. Just as USB-C provides a standardized way to connect your devices to various peripherals and accessories, MCP provides a standardized way to connect AI models to different data sources and tools.\n\nThe Agents SDK has support for MCP. This enables you to use a wide range of MCP servers to provide tools to your Agents.\n\n## MCP servers\n\nCurrently, the MCP spec defines three kinds of servers, based on the transport mechanism they use:\n\n1. **stdio** servers run as a subprocess of your application. You can think of them as running \"locally\".\n2. **HTTP over SSE** servers run remotely. You connect to them via a URL.\n3. **Streamable HTTP** servers run remotely using the Streamable HTTP transport defined in the MCP spec.\n\nYou can use the [`MCPServerStdio`](https://openai.github.io/openai-agents-python/ref/mcp/server/#agents.mcp.server.MCPServerStdio \"MCPServerStdio\"), [`MCPServerSse`](https://openai.github.io/openai-agents-python/ref/mcp/server/#agents.mcp.server.MCPServerSse \"MCPServerSse\"), and [`MCPServerStreamableHttp`](https://openai.github.io/openai-agents-python/ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp \"MCPServerStreamableHttp\") classes to connect to these servers.\n\nFor example, this is how you'd use the [official MCP filesystem server](https://www.npmjs.com/package/@modelcontextprotocol/server-filesystem).\n\n```md-code__content\nasync with MCPServerStdio(\n    params={\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", samples_dir],\n    }\n) as server:\n    tools = await server.list_tools()\n\n```\n\n## Using MCP servers\n\nMCP servers can be added to Agents. The Agents SDK will call `list_tools()` on the MCP servers each time the Agent is run. This makes the LLM aware of the MCP server's tools. When the LLM calls a tool from an MCP server, the SDK calls `call_tool()` on that server.\n\n```md-code__content\nagent=Agent(\n    name=\"Assistant\",\n    instructions=\"Use the tools to achieve the task\",\n    mcp_servers=[mcp_server_1, mcp_server_2]\n)\n\n```\n\n## Caching\n\nEvery time an Agent runs, it calls `list_tools()` on the MCP server. This can be a latency hit, especially if the server is a remote server. To automatically cache the list of tools, you can pass `cache_tools_list=True` to [`MCPServerStdio`](https://openai.github.io/openai-agents-python/ref/mcp/server/#agents.mcp.server.MCPServerStdio \"MCPServerStdio\"), [`MCPServerSse`](https://openai.github.io/openai-agents-python/ref/mcp/server/#agents.mcp.server.MCPServerSse \"MCPServerSse\"), and [`MCPServerStreamableHttp`](https://openai.github.io/openai-agents-python/ref/mcp/server/#agents.mcp.server.MCPServerStreamableHttp \"MCPServerStreamableHttp\"). You should only do this if you're certain the tool list will not change.\n\nIf you want to invalidate the cache, you can call `invalidate_tools_cache()` on the servers.\n\n## End-to-end examples\n\nView complete working examples at [examples/mcp](https://github.com/openai/openai-agents-python/tree/main/examples/mcp).\n\n## Tracing\n\n[Tracing](https://openai.github.io/openai-agents-python/tracing/) automatically captures MCP operations, including:\n\n1. Calls to the MCP server to list tools\n2. MCP-related info on function calls\n\n![MCP Tracing Screenshot](https://openai.github.io/openai-agents-python/assets/images/mcp-tracing.jpg)",
  "metadata": {
    "title": "Model context protocol (MCP) - OpenAI Agents SDK",
    "favicon": "https://openai.github.io/openai-agents-python/images/favicon-platform.svg",
    "language": "en",
    "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
    "viewport": "width=device-width,initial-scale=1",
    "scrapeId": "5fac32bc-4e6e-441c-ac63-acdd888ab5d4",
    "sourceURL": "https://openai.github.io/openai-agents-python/mcp/",
    "url": "https://openai.github.io/openai-agents-python/mcp/",
    "statusCode": 200,
    "contentType": "text/html; charset=utf-8",
    "proxyUsed": "basic"
  }
}