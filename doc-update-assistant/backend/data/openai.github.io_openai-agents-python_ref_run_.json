{
  "markdown": "[Skip to content](https://openai.github.io/openai-agents-python/ref/run/#runner)\n\n# `Runner`\n\n### Runner\n\nSource code in `src/agents/run.py`\n\n|     |     |\n| --- | --- |\n| ```<br>159<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>208<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>260<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>``` | ```md-code__content<br>class Runner:<br>    @classmethod<br>    async def run(<br>        cls,<br>        starting_agent: Agent[TContext],<br>        input: str | list[TResponseInputItem],<br>        *,<br>        context: TContext | None = None,<br>        max_turns: int = DEFAULT_MAX_TURNS,<br>        hooks: RunHooks[TContext] | None = None,<br>        run_config: RunConfig | None = None,<br>        previous_response_id: str | None = None,<br>    ) -> RunResult:<br>        \"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final<br>        output is generated. The loop runs like so:<br>        1. The agent is invoked with the given input.<br>        2. If there is a final output (i.e. the agent produces something of type<br>            `agent.output_type`, the loop terminates.<br>        3. If there's a handoff, we run the loop again, with the new agent.<br>        4. Else, we run tool calls (if any), and re-run the loop.<br>        In two cases, the agent may raise an exception:<br>        1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.<br>        2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.<br>        Note that only the first agent's input guardrails are run.<br>        Args:<br>            starting_agent: The starting agent to run.<br>            input: The initial input to the agent. You can pass a single string for a user message,<br>                or a list of input items.<br>            context: The context to run the agent with.<br>            max_turns: The maximum number of turns to run the agent for. A turn is defined as one<br>                AI invocation (including any tool calls that might occur).<br>            hooks: An object that receives callbacks on various lifecycle events.<br>            run_config: Global settings for the entire agent run.<br>            previous_response_id: The ID of the previous response, if using OpenAI models via the<br>                Responses API, this allows you to skip passing in input from the previous turn.<br>        Returns:<br>            A run result containing all the inputs, guardrail results and the output of the last<br>            agent. Agents may perform handoffs, so we don't know the specific type of the output.<br>        \"\"\"<br>        runner = DEFAULT_AGENT_RUNNER<br>        return await runner.run(<br>            starting_agent,<br>            input,<br>            context=context,<br>            max_turns=max_turns,<br>            hooks=hooks,<br>            run_config=run_config,<br>            previous_response_id=previous_response_id,<br>        )<br>    @classmethod<br>    def run_sync(<br>        cls,<br>        starting_agent: Agent[TContext],<br>        input: str | list[TResponseInputItem],<br>        *,<br>        context: TContext | None = None,<br>        max_turns: int = DEFAULT_MAX_TURNS,<br>        hooks: RunHooks[TContext] | None = None,<br>        run_config: RunConfig | None = None,<br>        previous_response_id: str | None = None,<br>    ) -> RunResult:<br>        \"\"\"Run a workflow synchronously, starting at the given agent. Note that this just wraps the<br>        `run` method, so it will not work if there's already an event loop (e.g. inside an async<br>        function, or in a Jupyter notebook or async context like FastAPI). For those cases, use<br>        the `run` method instead.<br>        The agent will run in a loop until a final output is generated. The loop runs like so:<br>        1. The agent is invoked with the given input.<br>        2. If there is a final output (i.e. the agent produces something of type<br>            `agent.output_type`, the loop terminates.<br>        3. If there's a handoff, we run the loop again, with the new agent.<br>        4. Else, we run tool calls (if any), and re-run the loop.<br>        In two cases, the agent may raise an exception:<br>        1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.<br>        2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.<br>        Note that only the first agent's input guardrails are run.<br>        Args:<br>            starting_agent: The starting agent to run.<br>            input: The initial input to the agent. You can pass a single string for a user message,<br>                or a list of input items.<br>            context: The context to run the agent with.<br>            max_turns: The maximum number of turns to run the agent for. A turn is defined as one<br>                AI invocation (including any tool calls that might occur).<br>            hooks: An object that receives callbacks on various lifecycle events.<br>            run_config: Global settings for the entire agent run.<br>            previous_response_id: The ID of the previous response, if using OpenAI models via the<br>                Responses API, this allows you to skip passing in input from the previous turn.<br>        Returns:<br>            A run result containing all the inputs, guardrail results and the output of the last<br>            agent. Agents may perform handoffs, so we don't know the specific type of the output.<br>        \"\"\"<br>        runner = DEFAULT_AGENT_RUNNER<br>        return runner.run_sync(<br>            starting_agent,<br>            input,<br>            context=context,<br>            max_turns=max_turns,<br>            hooks=hooks,<br>            run_config=run_config,<br>            previous_response_id=previous_response_id,<br>        )<br>    @classmethod<br>    def run_streamed(<br>        cls,<br>        starting_agent: Agent[TContext],<br>        input: str | list[TResponseInputItem],<br>        context: TContext | None = None,<br>        max_turns: int = DEFAULT_MAX_TURNS,<br>        hooks: RunHooks[TContext] | None = None,<br>        run_config: RunConfig | None = None,<br>        previous_response_id: str | None = None,<br>    ) -> RunResultStreaming:<br>        \"\"\"Run a workflow starting at the given agent in streaming mode. The returned result object<br>        contains a method you can use to stream semantic events as they are generated.<br>        The agent will run in a loop until a final output is generated. The loop runs like so:<br>        1. The agent is invoked with the given input.<br>        2. If there is a final output (i.e. the agent produces something of type<br>            `agent.output_type`, the loop terminates.<br>        3. If there's a handoff, we run the loop again, with the new agent.<br>        4. Else, we run tool calls (if any), and re-run the loop.<br>        In two cases, the agent may raise an exception:<br>        1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.<br>        2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.<br>        Note that only the first agent's input guardrails are run.<br>        Args:<br>            starting_agent: The starting agent to run.<br>            input: The initial input to the agent. You can pass a single string for a user message,<br>                or a list of input items.<br>            context: The context to run the agent with.<br>            max_turns: The maximum number of turns to run the agent for. A turn is defined as one<br>                AI invocation (including any tool calls that might occur).<br>            hooks: An object that receives callbacks on various lifecycle events.<br>            run_config: Global settings for the entire agent run.<br>            previous_response_id: The ID of the previous response, if using OpenAI models via the<br>                Responses API, this allows you to skip passing in input from the previous turn.<br>        Returns:<br>            A result object that contains data about the run, as well as a method to stream events.<br>        \"\"\"<br>        runner = DEFAULT_AGENT_RUNNER<br>        return runner.run_streamed(<br>            starting_agent,<br>            input,<br>            context=context,<br>            max_turns=max_turns,<br>            hooks=hooks,<br>            run_config=run_config,<br>            previous_response_id=previous_response_id,<br>        )<br>``` |\n\n#### run`async``classmethod`\n\n```md-code__content\nrun(\n    starting_agent: Agent[TContext],\n    input: str | list[TResponseInputItem],\n    *,\n    context: TContext | None = None,\n    max_turns: int = DEFAULT_MAX_TURNS,\n    hooks: RunHooks[TContext] | None = None,\n    run_config: RunConfig | None = None,\n    previous_response_id: str | None = None,\n) -> RunResult\n\n```\n\nRun a workflow starting at the given agent. The agent will run in a loop until a final\noutput is generated. The loop runs like so:\n1\\. The agent is invoked with the given input.\n2\\. If there is a final output (i.e. the agent produces something of type\n`agent.output_type`, the loop terminates.\n3\\. If there's a handoff, we run the loop again, with the new agent.\n4\\. Else, we run tool calls (if any), and re-run the loop.\nIn two cases, the agent may raise an exception:\n1\\. If the max\\_turns is exceeded, a MaxTurnsExceeded exception is raised.\n2\\. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.\nNote that only the first agent's input guardrails are run.\nArgs:\nstarting\\_agent: The starting agent to run.\ninput: The initial input to the agent. You can pass a single string for a user message,\nor a list of input items.\ncontext: The context to run the agent with.\nmax\\_turns: The maximum number of turns to run the agent for. A turn is defined as one\nAI invocation (including any tool calls that might occur).\nhooks: An object that receives callbacks on various lifecycle events.\nrun\\_config: Global settings for the entire agent run.\nprevious\\_response\\_id: The ID of the previous response, if using OpenAI models via the\nResponses API, this allows you to skip passing in input from the previous turn.\nReturns:\nA run result containing all the inputs, guardrail results and the output of the last\nagent. Agents may perform handoffs, so we don't know the specific type of the output.\n\nSource code in `src/agents/run.py`\n\n|     |     |\n| --- | --- |\n| ```<br>160<br>161<br>162<br>163<br>164<br>165<br>166<br>167<br>168<br>169<br>170<br>171<br>172<br>173<br>174<br>175<br>176<br>177<br>178<br>179<br>180<br>181<br>182<br>183<br>184<br>185<br>186<br>187<br>188<br>189<br>190<br>191<br>192<br>193<br>194<br>195<br>196<br>197<br>198<br>199<br>200<br>201<br>202<br>203<br>204<br>205<br>206<br>207<br>``` | ```md-code__content<br>@classmethod<br>async def run(<br>    cls,<br>    starting_agent: Agent[TContext],<br>    input: str | list[TResponseInputItem],<br>    *,<br>    context: TContext | None = None,<br>    max_turns: int = DEFAULT_MAX_TURNS,<br>    hooks: RunHooks[TContext] | None = None,<br>    run_config: RunConfig | None = None,<br>    previous_response_id: str | None = None,<br>) -> RunResult:<br>    \"\"\"Run a workflow starting at the given agent. The agent will run in a loop until a final<br>    output is generated. The loop runs like so:<br>    1. The agent is invoked with the given input.<br>    2. If there is a final output (i.e. the agent produces something of type<br>        `agent.output_type`, the loop terminates.<br>    3. If there's a handoff, we run the loop again, with the new agent.<br>    4. Else, we run tool calls (if any), and re-run the loop.<br>    In two cases, the agent may raise an exception:<br>    1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.<br>    2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.<br>    Note that only the first agent's input guardrails are run.<br>    Args:<br>        starting_agent: The starting agent to run.<br>        input: The initial input to the agent. You can pass a single string for a user message,<br>            or a list of input items.<br>        context: The context to run the agent with.<br>        max_turns: The maximum number of turns to run the agent for. A turn is defined as one<br>            AI invocation (including any tool calls that might occur).<br>        hooks: An object that receives callbacks on various lifecycle events.<br>        run_config: Global settings for the entire agent run.<br>        previous_response_id: The ID of the previous response, if using OpenAI models via the<br>            Responses API, this allows you to skip passing in input from the previous turn.<br>    Returns:<br>        A run result containing all the inputs, guardrail results and the output of the last<br>        agent. Agents may perform handoffs, so we don't know the specific type of the output.<br>    \"\"\"<br>    runner = DEFAULT_AGENT_RUNNER<br>    return await runner.run(<br>        starting_agent,<br>        input,<br>        context=context,<br>        max_turns=max_turns,<br>        hooks=hooks,<br>        run_config=run_config,<br>        previous_response_id=previous_response_id,<br>    )<br>``` |\n\n#### run\\_sync`classmethod`\n\n```md-code__content\nrun_sync(\n    starting_agent: Agent[TContext],\n    input: str | list[TResponseInputItem],\n    *,\n    context: TContext | None = None,\n    max_turns: int = DEFAULT_MAX_TURNS,\n    hooks: RunHooks[TContext] | None = None,\n    run_config: RunConfig | None = None,\n    previous_response_id: str | None = None,\n) -> RunResult\n\n```\n\nRun a workflow synchronously, starting at the given agent. Note that this just wraps the\n`run` method, so it will not work if there's already an event loop (e.g. inside an async\nfunction, or in a Jupyter notebook or async context like FastAPI). For those cases, use\nthe `run` method instead.\nThe agent will run in a loop until a final output is generated. The loop runs like so:\n1\\. The agent is invoked with the given input.\n2\\. If there is a final output (i.e. the agent produces something of type\n`agent.output_type`, the loop terminates.\n3\\. If there's a handoff, we run the loop again, with the new agent.\n4\\. Else, we run tool calls (if any), and re-run the loop.\nIn two cases, the agent may raise an exception:\n1\\. If the max\\_turns is exceeded, a MaxTurnsExceeded exception is raised.\n2\\. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.\nNote that only the first agent's input guardrails are run.\nArgs:\nstarting\\_agent: The starting agent to run.\ninput: The initial input to the agent. You can pass a single string for a user message,\nor a list of input items.\ncontext: The context to run the agent with.\nmax\\_turns: The maximum number of turns to run the agent for. A turn is defined as one\nAI invocation (including any tool calls that might occur).\nhooks: An object that receives callbacks on various lifecycle events.\nrun\\_config: Global settings for the entire agent run.\nprevious\\_response\\_id: The ID of the previous response, if using OpenAI models via the\nResponses API, this allows you to skip passing in input from the previous turn.\nReturns:\nA run result containing all the inputs, guardrail results and the output of the last\nagent. Agents may perform handoffs, so we don't know the specific type of the output.\n\nSource code in `src/agents/run.py`\n\n|     |     |\n| --- | --- |\n| ```<br>209<br>210<br>211<br>212<br>213<br>214<br>215<br>216<br>217<br>218<br>219<br>220<br>221<br>222<br>223<br>224<br>225<br>226<br>227<br>228<br>229<br>230<br>231<br>232<br>233<br>234<br>235<br>236<br>237<br>238<br>239<br>240<br>241<br>242<br>243<br>244<br>245<br>246<br>247<br>248<br>249<br>250<br>251<br>252<br>253<br>254<br>255<br>256<br>257<br>258<br>259<br>``` | ```md-code__content<br>@classmethod<br>def run_sync(<br>    cls,<br>    starting_agent: Agent[TContext],<br>    input: str | list[TResponseInputItem],<br>    *,<br>    context: TContext | None = None,<br>    max_turns: int = DEFAULT_MAX_TURNS,<br>    hooks: RunHooks[TContext] | None = None,<br>    run_config: RunConfig | None = None,<br>    previous_response_id: str | None = None,<br>) -> RunResult:<br>    \"\"\"Run a workflow synchronously, starting at the given agent. Note that this just wraps the<br>    `run` method, so it will not work if there's already an event loop (e.g. inside an async<br>    function, or in a Jupyter notebook or async context like FastAPI). For those cases, use<br>    the `run` method instead.<br>    The agent will run in a loop until a final output is generated. The loop runs like so:<br>    1. The agent is invoked with the given input.<br>    2. If there is a final output (i.e. the agent produces something of type<br>        `agent.output_type`, the loop terminates.<br>    3. If there's a handoff, we run the loop again, with the new agent.<br>    4. Else, we run tool calls (if any), and re-run the loop.<br>    In two cases, the agent may raise an exception:<br>    1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.<br>    2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.<br>    Note that only the first agent's input guardrails are run.<br>    Args:<br>        starting_agent: The starting agent to run.<br>        input: The initial input to the agent. You can pass a single string for a user message,<br>            or a list of input items.<br>        context: The context to run the agent with.<br>        max_turns: The maximum number of turns to run the agent for. A turn is defined as one<br>            AI invocation (including any tool calls that might occur).<br>        hooks: An object that receives callbacks on various lifecycle events.<br>        run_config: Global settings for the entire agent run.<br>        previous_response_id: The ID of the previous response, if using OpenAI models via the<br>            Responses API, this allows you to skip passing in input from the previous turn.<br>    Returns:<br>        A run result containing all the inputs, guardrail results and the output of the last<br>        agent. Agents may perform handoffs, so we don't know the specific type of the output.<br>    \"\"\"<br>    runner = DEFAULT_AGENT_RUNNER<br>    return runner.run_sync(<br>        starting_agent,<br>        input,<br>        context=context,<br>        max_turns=max_turns,<br>        hooks=hooks,<br>        run_config=run_config,<br>        previous_response_id=previous_response_id,<br>    )<br>``` |\n\n#### run\\_streamed`classmethod`\n\n```md-code__content\nrun_streamed(\n    starting_agent: Agent[TContext],\n    input: str | list[TResponseInputItem],\n    context: TContext | None = None,\n    max_turns: int = DEFAULT_MAX_TURNS,\n    hooks: RunHooks[TContext] | None = None,\n    run_config: RunConfig | None = None,\n    previous_response_id: str | None = None,\n) -> RunResultStreaming\n\n```\n\nRun a workflow starting at the given agent in streaming mode. The returned result object\ncontains a method you can use to stream semantic events as they are generated.\nThe agent will run in a loop until a final output is generated. The loop runs like so:\n1\\. The agent is invoked with the given input.\n2\\. If there is a final output (i.e. the agent produces something of type\n`agent.output_type`, the loop terminates.\n3\\. If there's a handoff, we run the loop again, with the new agent.\n4\\. Else, we run tool calls (if any), and re-run the loop.\nIn two cases, the agent may raise an exception:\n1\\. If the max\\_turns is exceeded, a MaxTurnsExceeded exception is raised.\n2\\. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.\nNote that only the first agent's input guardrails are run.\nArgs:\nstarting\\_agent: The starting agent to run.\ninput: The initial input to the agent. You can pass a single string for a user message,\nor a list of input items.\ncontext: The context to run the agent with.\nmax\\_turns: The maximum number of turns to run the agent for. A turn is defined as one\nAI invocation (including any tool calls that might occur).\nhooks: An object that receives callbacks on various lifecycle events.\nrun\\_config: Global settings for the entire agent run.\nprevious\\_response\\_id: The ID of the previous response, if using OpenAI models via the\nResponses API, this allows you to skip passing in input from the previous turn.\nReturns:\nA result object that contains data about the run, as well as a method to stream events.\n\nSource code in `src/agents/run.py`\n\n|     |     |\n| --- | --- |\n| ```<br>261<br>262<br>263<br>264<br>265<br>266<br>267<br>268<br>269<br>270<br>271<br>272<br>273<br>274<br>275<br>276<br>277<br>278<br>279<br>280<br>281<br>282<br>283<br>284<br>285<br>286<br>287<br>288<br>289<br>290<br>291<br>292<br>293<br>294<br>295<br>296<br>297<br>298<br>299<br>300<br>301<br>302<br>303<br>304<br>305<br>306<br>307<br>``` | ```md-code__content<br>@classmethod<br>def run_streamed(<br>    cls,<br>    starting_agent: Agent[TContext],<br>    input: str | list[TResponseInputItem],<br>    context: TContext | None = None,<br>    max_turns: int = DEFAULT_MAX_TURNS,<br>    hooks: RunHooks[TContext] | None = None,<br>    run_config: RunConfig | None = None,<br>    previous_response_id: str | None = None,<br>) -> RunResultStreaming:<br>    \"\"\"Run a workflow starting at the given agent in streaming mode. The returned result object<br>    contains a method you can use to stream semantic events as they are generated.<br>    The agent will run in a loop until a final output is generated. The loop runs like so:<br>    1. The agent is invoked with the given input.<br>    2. If there is a final output (i.e. the agent produces something of type<br>        `agent.output_type`, the loop terminates.<br>    3. If there's a handoff, we run the loop again, with the new agent.<br>    4. Else, we run tool calls (if any), and re-run the loop.<br>    In two cases, the agent may raise an exception:<br>    1. If the max_turns is exceeded, a MaxTurnsExceeded exception is raised.<br>    2. If a guardrail tripwire is triggered, a GuardrailTripwireTriggered exception is raised.<br>    Note that only the first agent's input guardrails are run.<br>    Args:<br>        starting_agent: The starting agent to run.<br>        input: The initial input to the agent. You can pass a single string for a user message,<br>            or a list of input items.<br>        context: The context to run the agent with.<br>        max_turns: The maximum number of turns to run the agent for. A turn is defined as one<br>            AI invocation (including any tool calls that might occur).<br>        hooks: An object that receives callbacks on various lifecycle events.<br>        run_config: Global settings for the entire agent run.<br>        previous_response_id: The ID of the previous response, if using OpenAI models via the<br>            Responses API, this allows you to skip passing in input from the previous turn.<br>    Returns:<br>        A result object that contains data about the run, as well as a method to stream events.<br>    \"\"\"<br>    runner = DEFAULT_AGENT_RUNNER<br>    return runner.run_streamed(<br>        starting_agent,<br>        input,<br>        context=context,<br>        max_turns=max_turns,<br>        hooks=hooks,<br>        run_config=run_config,<br>        previous_response_id=previous_response_id,<br>    )<br>``` |\n\n### RunConfig`dataclass`\n\nConfigures settings for the entire agent run.\n\nSource code in `src/agents/run.py`\n\n|     |     |\n| --- | --- |\n| ```<br> 81<br> 82<br> 83<br> 84<br> 85<br> 86<br> 87<br> 88<br> 89<br> 90<br> 91<br> 92<br> 93<br> 94<br> 95<br> 96<br> 97<br> 98<br> 99<br>100<br>101<br>102<br>103<br>104<br>105<br>106<br>107<br>108<br>109<br>110<br>111<br>112<br>113<br>114<br>115<br>116<br>117<br>118<br>119<br>120<br>121<br>122<br>123<br>124<br>125<br>126<br>127<br>128<br>129<br>130<br>131<br>132<br>133<br>134<br>135<br>136<br>137<br>``` | ```md-code__content<br>@dataclass<br>class RunConfig:<br>    \"\"\"Configures settings for the entire agent run.\"\"\"<br>    model: str | Model | None = None<br>    \"\"\"The model to use for the entire agent run. If set, will override the model set on every<br>    agent. The model_provider passed in below must be able to resolve this model name.<br>    \"\"\"<br>    model_provider: ModelProvider = field(default_factory=MultiProvider)<br>    \"\"\"The model provider to use when looking up string model names. Defaults to OpenAI.\"\"\"<br>    model_settings: ModelSettings | None = None<br>    \"\"\"Configure global model settings. Any non-null values will override the agent-specific model<br>    settings.<br>    \"\"\"<br>    handoff_input_filter: HandoffInputFilter | None = None<br>    \"\"\"A global input filter to apply to all handoffs. If `Handoff.input_filter` is set, then that<br>    will take precedence. The input filter allows you to edit the inputs that are sent to the new<br>    agent. See the documentation in `Handoff.input_filter` for more details.<br>    \"\"\"<br>    input_guardrails: list[InputGuardrail[Any]] | None = None<br>    \"\"\"A list of input guardrails to run on the initial run input.\"\"\"<br>    output_guardrails: list[OutputGuardrail[Any]] | None = None<br>    \"\"\"A list of output guardrails to run on the final output of the run.\"\"\"<br>    tracing_disabled: bool = False<br>    \"\"\"Whether tracing is disabled for the agent run. If disabled, we will not trace the agent run.<br>    \"\"\"<br>    trace_include_sensitive_data: bool = True<br>    \"\"\"Whether we include potentially sensitive data (for example: inputs/outputs of tool calls or<br>    LLM generations) in traces. If False, we'll still create spans for these events, but the<br>    sensitive data will not be included.<br>    \"\"\"<br>    workflow_name: str = \"Agent workflow\"<br>    \"\"\"The name of the run, used for tracing. Should be a logical name for the run, like<br>    \"Code generation workflow\" or \"Customer support agent\".<br>    \"\"\"<br>    trace_id: str | None = None<br>    \"\"\"A custom trace ID to use for tracing. If not provided, we will generate a new trace ID.\"\"\"<br>    group_id: str | None = None<br>    \"\"\"<br>    A grouping identifier to use for tracing, to link multiple traces from the same conversation<br>    or process. For example, you might use a chat thread ID.<br>    \"\"\"<br>    trace_metadata: dict[str, Any] | None = None<br>    \"\"\"<br>    An optional dictionary of additional metadata to include with the trace.<br>    \"\"\"<br>``` |\n\n#### model`class-attribute``instance-attribute`\n\n```md-code__content\nmodel: str | Model | None = None\n\n```\n\nThe model to use for the entire agent run. If set, will override the model set on every\nagent. The model\\_provider passed in below must be able to resolve this model name.\n\n#### model\\_provider`class-attribute``instance-attribute`\n\n```md-code__content\nmodel_provider: ModelProvider = field(\n    default_factory=MultiProvider\n)\n\n```\n\nThe model provider to use when looking up string model names. Defaults to OpenAI.\n\n#### model\\_settings`class-attribute``instance-attribute`\n\n```md-code__content\nmodel_settings: ModelSettings | None = None\n\n```\n\nConfigure global model settings. Any non-null values will override the agent-specific model\nsettings.\n\n#### handoff\\_input\\_filter`class-attribute``instance-attribute`\n\n```md-code__content\nhandoff_input_filter: HandoffInputFilter | None = None\n\n```\n\nA global input filter to apply to all handoffs. If `Handoff.input_filter` is set, then that\nwill take precedence. The input filter allows you to edit the inputs that are sent to the new\nagent. See the documentation in `Handoff.input_filter` for more details.\n\n#### input\\_guardrails`class-attribute``instance-attribute`\n\n```md-code__content\ninput_guardrails: list[InputGuardrail[Any]] | None = None\n\n```\n\nA list of input guardrails to run on the initial run input.\n\n#### output\\_guardrails`class-attribute``instance-attribute`\n\n```md-code__content\noutput_guardrails: list[OutputGuardrail[Any]] | None = None\n\n```\n\nA list of output guardrails to run on the final output of the run.\n\n#### tracing\\_disabled`class-attribute``instance-attribute`\n\n```md-code__content\ntracing_disabled: bool = False\n\n```\n\nWhether tracing is disabled for the agent run. If disabled, we will not trace the agent run.\n\n#### trace\\_include\\_sensitive\\_data`class-attribute``instance-attribute`\n\n```md-code__content\ntrace_include_sensitive_data: bool = True\n\n```\n\nWhether we include potentially sensitive data (for example: inputs/outputs of tool calls or\nLLM generations) in traces. If False, we'll still create spans for these events, but the\nsensitive data will not be included.\n\n#### workflow\\_name`class-attribute``instance-attribute`\n\n```md-code__content\nworkflow_name: str = 'Agent workflow'\n\n```\n\nThe name of the run, used for tracing. Should be a logical name for the run, like\n\"Code generation workflow\" or \"Customer support agent\".\n\n#### trace\\_id`class-attribute``instance-attribute`\n\n```md-code__content\ntrace_id: str | None = None\n\n```\n\nA custom trace ID to use for tracing. If not provided, we will generate a new trace ID.\n\n#### group\\_id`class-attribute``instance-attribute`\n\n```md-code__content\ngroup_id: str | None = None\n\n```\n\nA grouping identifier to use for tracing, to link multiple traces from the same conversation\nor process. For example, you might use a chat thread ID.\n\n#### trace\\_metadata`class-attribute``instance-attribute`\n\n```md-code__content\ntrace_metadata: dict[str, Any] | None = None\n\n```\n\nAn optional dictionary of additional metadata to include with the trace.",
  "metadata": {
    "favicon": "https://openai.github.io/openai-agents-python/images/favicon-platform.svg",
    "title": "Runner - OpenAI Agents SDK",
    "language": "en",
    "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
    "viewport": "width=device-width,initial-scale=1",
    "scrapeId": "c761183c-0174-4241-a05d-cf675b443120",
    "sourceURL": "https://openai.github.io/openai-agents-python/ref/run/",
    "url": "https://openai.github.io/openai-agents-python/ref/run/",
    "statusCode": 200,
    "contentType": "text/html; charset=utf-8",
    "proxyUsed": "basic"
  }
}