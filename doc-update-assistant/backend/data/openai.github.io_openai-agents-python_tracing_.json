{
  "markdown": "[Skip to content](https://openai.github.io/openai-agents-python/tracing/#tracing)\n\n# Tracing\n\nThe Agents SDK includes built-in tracing, collecting a comprehensive record of events during an agent run: LLM generations, tool calls, handoffs, guardrails, and even custom events that occur. Using the [Traces dashboard](https://platform.openai.com/traces), you can debug, visualize, and monitor your workflows during development and in production.\n\nNote\n\nTracing is enabled by default. There are two ways to disable tracing:\n\n1. You can globally disable tracing by setting the env var `OPENAI_AGENTS_DISABLE_TRACING=1`\n2. You can disable tracing for a single run by setting [`agents.run.RunConfig.tracing_disabled`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.tracing_disabled \"tracing_disabled            class-attribute       instance-attribute   \") to `True`\n\n**_For organizations operating under a Zero Data Retention (ZDR) policy using OpenAI's APIs, tracing is unavailable._**\n\n## Traces and spans\n\n- **Traces** represent a single end-to-end operation of a \"workflow\". They're composed of Spans. Traces have the following properties:\n  - `workflow_name`: This is the logical workflow or app. For example \"Code generation\" or \"Customer service\".\n  - `trace_id`: A unique ID for the trace. Automatically generated if you don't pass one. Must have the format `trace_<32_alphanumeric>`.\n  - `group_id`: Optional group ID, to link multiple traces from the same conversation. For example, you might use a chat thread ID.\n  - `disabled`: If True, the trace will not be recorded.\n  - `metadata`: Optional metadata for the trace.\n- **Spans** represent operations that have a start and end time. Spans have:\n  - `started_at` and `ended_at` timestamps.\n  - `trace_id`, to represent the trace they belong to\n  - `parent_id`, which points to the parent Span of this Span (if any)\n  - `span_data`, which is information about the Span. For example, `AgentSpanData` contains information about the Agent, `GenerationSpanData` contains information about the LLM generation, etc.\n\n## Default tracing\n\nBy default, the SDK traces the following:\n\n- The entire `Runner.{run, run_sync, run_streamed}()` is wrapped in a `trace()`.\n- Each time an agent runs, it is wrapped in `agent_span()`\n- LLM generations are wrapped in `generation_span()`\n- Function tool calls are each wrapped in `function_span()`\n- Guardrails are wrapped in `guardrail_span()`\n- Handoffs are wrapped in `handoff_span()`\n- Audio inputs (speech-to-text) are wrapped in a `transcription_span()`\n- Audio outputs (text-to-speech) are wrapped in a `speech_span()`\n- Related audio spans may be parented under a `speech_group_span()`\n\nBy default, the trace is named \"Agent trace\". You can set this name if you use `trace`, or you can can configure the name and other properties with the [`RunConfig`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig \"RunConfig            dataclass   \").\n\nIn addition, you can set up [custom trace processors](https://openai.github.io/openai-agents-python/tracing/#custom-tracing-processors) to push traces to other destinations (as a replacement, or secondary destination).\n\n## Higher level traces\n\nSometimes, you might want multiple calls to `run()` to be part of a single trace. You can do this by wrapping the entire code in a `trace()`.\n\n```md-code__content\nfrom agents import Agent, Runner, trace\n\nasync def main():\n    agent = Agent(name=\"Joke generator\", instructions=\"Tell funny jokes.\")\n\n    with trace(\"Joke workflow\"):\n        first_result = await Runner.run(agent, \"Tell me a joke\")\n        second_result = await Runner.run(agent, f\"Rate this joke: {first_result.final_output}\")\n        print(f\"Joke: {first_result.final_output}\")\n        print(f\"Rating: {second_result.final_output}\")\n\n```\n\n## Creating traces\n\nYou can use the [`trace()`](https://openai.github.io/openai-agents-python/ref/tracing/#agents.tracing.trace \"trace\") function to create a trace. Traces need to be started and finished. You have two options to do so:\n\n1. **Recommended**: use the trace as a context manager, i.e. `with trace(...) as my_trace`. This will automatically start and end the trace at the right time.\n2. You can also manually call [`trace.start()`](https://openai.github.io/openai-agents-python/ref/tracing/#agents.tracing.Trace.start \"start            abstractmethod   \") and [`trace.finish()`](https://openai.github.io/openai-agents-python/ref/tracing/#agents.tracing.Trace.finish \"finish            abstractmethod   \").\n\nThe current trace is tracked via a Python [`contextvar`](https://docs.python.org/3/library/contextvars.html). This means that it works with concurrency automatically. If you manually start/end a trace, you'll need to pass `mark_as_current` and `reset_current` to `start()`/ `finish()` to update the current trace.\n\n## Creating spans\n\nYou can use the various [`*_span()`](https://openai.github.io/openai-agents-python/ref/tracing/create/#agents.tracing.create) methods to create a span. In general, you don't need to manually create spans. A [`custom_span()`](https://openai.github.io/openai-agents-python/ref/tracing/#agents.tracing.custom_span \"custom_span\") function is available for tracking custom span information.\n\nSpans are automatically part of the current trace, and are nested under the nearest current span, which is tracked via a Python [`contextvar`](https://docs.python.org/3/library/contextvars.html).\n\n## Sensitive data\n\nCertain spans may capture potentially sensitive data.\n\nThe `generation_span()` stores the inputs/outputs of the LLM generation, and `function_span()` stores the inputs/outputs of function calls. These may contain sensitive data, so you can disable capturing that data via [`RunConfig.trace_include_sensitive_data`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.trace_include_sensitive_data \"trace_include_sensitive_data            class-attribute       instance-attribute   \").\n\nSimilarly, Audio spans include base64-encoded PCM data for input and output audio by default. You can disable capturing this audio data by configuring [`VoicePipelineConfig.trace_include_sensitive_audio_data`](https://openai.github.io/openai-agents-python/ref/voice/pipeline_config/#agents.voice.pipeline_config.VoicePipelineConfig.trace_include_sensitive_audio_data \"trace_include_sensitive_audio_data            class-attribute       instance-attribute   \").\n\n## Custom tracing processors\n\nThe high level architecture for tracing is:\n\n- At initialization, we create a global [`TraceProvider`](https://openai.github.io/openai-agents-python/ref/tracing/#agents.tracing.TraceProvider \"TraceProvider\"), which is responsible for creating traces.\n- We configure the `TraceProvider` with a [`BatchTraceProcessor`](https://openai.github.io/openai-agents-python/ref/tracing/processors/#agents.tracing.processors.BatchTraceProcessor \"BatchTraceProcessor\") that sends traces/spans in batches to a [`BackendSpanExporter`](https://openai.github.io/openai-agents-python/ref/tracing/processors/#agents.tracing.processors.BackendSpanExporter \"BackendSpanExporter\"), which exports the spans and traces to the OpenAI backend in batches.\n\nTo customize this default setup, to send traces to alternative or additional backends or modifying exporter behavior, you have two options:\n\n1. [`add_trace_processor()`](https://openai.github.io/openai-agents-python/ref/tracing/#agents.tracing.add_trace_processor \"add_trace_processor\") lets you add an **additional** trace processor that will receive traces and spans as they are ready. This lets you do your own processing in addition to sending traces to OpenAI's backend.\n2. [`set_trace_processors()`](https://openai.github.io/openai-agents-python/ref/tracing/#agents.tracing.set_trace_processors \"set_trace_processors\") lets you **replace** the default processors with your own trace processors. This means traces will not be sent to the OpenAI backend unless you include a `TracingProcessor` that does so.\n\n## External tracing processors list\n\n- [Weights & Biases](https://weave-docs.wandb.ai/guides/integrations/openai_agents)\n- [Arize-Phoenix](https://docs.arize.com/phoenix/tracing/integrations-tracing/openai-agents-sdk)\n- [Future AGI](https://docs.futureagi.com/future-agi/products/observability/auto-instrumentation/openai_agents)\n- [MLflow (self-hosted/OSS](https://mlflow.org/docs/latest/tracing/integrations/openai-agent)\n- [MLflow (Databricks hosted](https://docs.databricks.com/aws/en/mlflow/mlflow-tracing#-automatic-tracing)\n- [Braintrust](https://braintrust.dev/docs/guides/traces/integrations#openai-agents-sdk)\n- [Pydantic Logfire](https://logfire.pydantic.dev/docs/integrations/llms/openai/#openai-agents)\n- [AgentOps](https://docs.agentops.ai/v1/integrations/agentssdk)\n- [Scorecard](https://docs.scorecard.io/docs/documentation/features/tracing#openai-agents-sdk-integration)\n- [Keywords AI](https://docs.keywordsai.co/integration/development-frameworks/openai-agent)\n- [LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk)\n- [Maxim AI](https://www.getmaxim.ai/docs/observe/integrations/openai-agents-sdk)\n- [Comet Opik](https://www.comet.com/docs/opik/tracing/integrations/openai_agents)\n- [Langfuse](https://langfuse.com/docs/integrations/openaiagentssdk/openai-agents)\n- [Langtrace](https://docs.langtrace.ai/supported-integrations/llm-frameworks/openai-agents-sdk)\n- [Okahu-Monocle](https://github.com/monocle2ai/monocle)\n- [Galileo](https://v2docs.galileo.ai/integrations/openai-agent-integration#openai-agent-integration)\n- [Portkey AI](https://portkey.ai/docs/integrations/agents/openai-agents)",
  "metadata": {
    "title": "Tracing - OpenAI Agents SDK",
    "favicon": "https://openai.github.io/openai-agents-python/images/favicon-platform.svg",
    "language": "en",
    "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
    "viewport": "width=device-width,initial-scale=1",
    "scrapeId": "9634d61a-2400-4cc4-95ef-ceb88cebe4f0",
    "sourceURL": "https://openai.github.io/openai-agents-python/tracing/",
    "url": "https://openai.github.io/openai-agents-python/tracing/",
    "statusCode": 200,
    "contentType": "text/html; charset=utf-8",
    "proxyUsed": "basic"
  }
}