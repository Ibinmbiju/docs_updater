{
  "markdown": "[Skip to content](https://openai.github.io/openai-agents-python/streaming/#streaming)\n\n# Streaming\n\nStreaming lets you subscribe to updates of the agent run as it proceeds. This can be useful for showing the end-user progress updates and partial responses.\n\nTo stream, you can call [`Runner.run_streamed()`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.Runner.run_streamed \"run_streamed            classmethod   \"), which will give you a [`RunResultStreaming`](https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResultStreaming \"RunResultStreaming            dataclass   \"). Calling `result.stream_events()` gives you an async stream of [`StreamEvent`](https://openai.github.io/openai-agents-python/ref/stream_events/#agents.stream_events.StreamEvent \"StreamEvent            module-attribute   \") objects, which are described below.\n\n## Raw response events\n\n[`RawResponsesStreamEvent`](https://openai.github.io/openai-agents-python/ref/stream_events/#agents.stream_events.RawResponsesStreamEvent \"RawResponsesStreamEvent            dataclass   \") are raw events passed directly from the LLM. They are in OpenAI Responses API format, which means each event has a type (like `response.created`, `response.output_text.delta`, etc) and data. These events are useful if you want to stream response messages to the user as soon as they are generated.\n\nFor example, this will output the text generated by the LLM token-by-token.\n\n```md-code__content\nimport asyncio\nfrom openai.types.responses import ResponseTextDeltaEvent\nfrom agents import Agent, Runner\n\nasync def main():\n    agent = Agent(\n        name=\"Joker\",\n        instructions=\"You are a helpful assistant.\",\n    )\n\n    result = Runner.run_streamed(agent, input=\"Please tell me 5 jokes.\")\n    async for event in result.stream_events():\n        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n            print(event.data.delta, end=\"\", flush=True)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n```\n\n## Run item events and agent events\n\n[`RunItemStreamEvent`](https://openai.github.io/openai-agents-python/ref/stream_events/#agents.stream_events.RunItemStreamEvent \"RunItemStreamEvent            dataclass   \") s are higher level events. They inform you when an item has been fully generated. This allows you to push progress updates at the level of \"message generated\", \"tool ran\", etc, instead of each token. Similarly, [`AgentUpdatedStreamEvent`](https://openai.github.io/openai-agents-python/ref/stream_events/#agents.stream_events.AgentUpdatedStreamEvent \"AgentUpdatedStreamEvent            dataclass   \") gives you updates when the current agent changes (e.g. as the result of a handoff).\n\nFor example, this will ignore raw events and stream updates to the user.\n\n```md-code__content\nimport asyncio\nimport random\nfrom agents import Agent, ItemHelpers, Runner, function_tool\n\n@function_tool\ndef how_many_jokes() -> int:\n    return random.randint(1, 10)\n\nasync def main():\n    agent = Agent(\n        name=\"Joker\",\n        instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n        tools=[how_many_jokes],\n    )\n\n    result = Runner.run_streamed(\n        agent,\n        input=\"Hello\",\n    )\n    print(\"=== Run starting ===\")\n\n    async for event in result.stream_events():\n        # We'll ignore the raw responses event deltas\n        if event.type == \"raw_response_event\":\n            continue\n        # When the agent updates, print that\n        elif event.type == \"agent_updated_stream_event\":\n            print(f\"Agent updated: {event.new_agent.name}\")\n            continue\n        # When items are generated, print them\n        elif event.type == \"run_item_stream_event\":\n            if event.item.type == \"tool_call_item\":\n                print(\"-- Tool was called\")\n            elif event.item.type == \"tool_call_output_item\":\n                print(f\"-- Tool output: {event.item.output}\")\n            elif event.item.type == \"message_output_item\":\n                print(f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\")\n            else:\n                pass  # Ignore other event types\n\n    print(\"=== Run complete ===\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n```",
  "metadata": {
    "title": "Streaming - OpenAI Agents SDK",
    "language": "en",
    "viewport": "width=device-width,initial-scale=1",
    "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
    "favicon": "https://openai.github.io/openai-agents-python/images/favicon-platform.svg",
    "scrapeId": "aa76a1b2-5949-4927-a98d-40aca476c9a5",
    "sourceURL": "https://openai.github.io/openai-agents-python/streaming/",
    "url": "https://openai.github.io/openai-agents-python/streaming/",
    "statusCode": 200,
    "contentType": "text/html; charset=utf-8",
    "proxyUsed": "basic"
  }
}